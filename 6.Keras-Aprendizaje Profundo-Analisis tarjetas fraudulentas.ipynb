{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalaciones preliminares\n",
    "*Antes de comenzar estas lineas de código debe instalar las ultimas versiones de los paquetes*\n",
    "- Scikit-learn: Buscando en anaconda scikit-learn o poniendo en el terminal conda install scikit-learn o pip install scikit-learn ---> Más info en https://scikit-learn.org/stable/install.html\n",
    "- Pandas: Buscando en anaconda pandas o poniendo en el terminal conda install pandas o pip install pandas ---> Más info en https://pandas.pydata.org/docs/getting_started/index.html\n",
    "- Numpy: Buscando en anaconda numpy o poniendo en el terminal conda install numpy o pip install numpy ---> Más info en https://numpy.org/install/\n",
    "- Imbalanced-learn: Poniendo en el terminal conda install -c conda-forge imbalanced-learn o pip install -U imbalanced-learn ---> Más info en https://imbalanced-learn.readthedocs.io/en/stable/install.html\n",
    "- Pickle: Poniendo en el terminal pip install pickle ---> Más info si no te funciona en https://stackoverflow.com/questions/48477949/not-able-to-pip-install-pickle-in-python-3-6/48477988\n",
    "- seaborn: Buscando en anaconda seaborn o poniendo en el terminal pip install seaborn o conda install seaborn ---> Más info en https://seaborn.pydata.org/installing.html\n",
    "- matplotlib: Buscando en anaconda matplotlib o poniendo en el terminal pip install -U matplotlib ---> Más info en https://matplotlib.org/users/installing.html\n",
    "- Keras: Buscando e instalando en anaconda los módulos de tensorflow y keras. También se pueden instalar usando los comandos pip install tensorflow y pip install keras ---> Más infor en https://keras.io/about/\n",
    "\n",
    "Base de datos obtenida de: https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura y balanceo de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabecera: \"Time\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"\n",
      "Ejemplo de caracteristicas: [0.0, -1.3598071336738, -0.0727811733098497, 2.53634673796914, 1.37815522427443, -0.338320769942518, 0.462387777762292, 0.239598554061257, 0.0986979012610507, 0.363786969611213, 0.0907941719789316, -0.551599533260813, -0.617800855762348, -0.991389847235408, -0.311169353699879, 1.46817697209427, -0.470400525259478, 0.207971241929242, 0.0257905801985591, 0.403992960255733, 0.251412098239705, -0.018306777944153, 0.277837575558899, -0.110473910188767, 0.0669280749146731, 0.128539358273528, -0.189114843888824, 0.133558376740387, -0.0210530534538215, 149.62]\n",
      "Forma de los datos de entrada al modelo: (284807, 30)\n",
      "Forma de las clases de salida: (284807, 1)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "#Nombre del fichero que quiero leer\n",
    "datos = \"creditcard.csv\"\n",
    "\n",
    "#Inicialización de los arrays\n",
    "caracteristicas = []\n",
    "clase = []\n",
    "\n",
    "#Lectura de datos\n",
    "with open(datos) as f:\n",
    "    for i, linea in enumerate(f):\n",
    "        #Saltamos la cabecera del csv\n",
    "        if i == 0:\n",
    "            print(\"Cabecera:\", linea.strip())\n",
    "            continue  \n",
    "        #Introducimos los datos en los arrays de características\n",
    "        campos = linea.strip().split(\",\")\n",
    "        caracteristicas.append([float(v.replace('\"', \"\")) for v in campos[:-1]])\n",
    "        clase.append([int(campos[-1].replace('\"', \"\"))])\n",
    "        if i == 1:\n",
    "            print(\"Ejemplo de caracteristicas:\", caracteristicas[-1])\n",
    "\n",
    "#Conversión de datos\n",
    "data = np.array(caracteristicas, dtype=\"float32\")\n",
    "target = np.array(clase, dtype=\"uint8\")\n",
    "\n",
    "#Pintamos la forma de los datos\n",
    "print(\"Forma de los datos de entrada al modelo:\", data.shape)\n",
    "print(\"Forma de las clases de salida:\", target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarjetas legales: 284315\n",
      "Tarjetas fraudulentas: 492\n"
     ]
    }
   ],
   "source": [
    "#Primero analizamos los datos\n",
    "\n",
    "#Inicializacion de la cuenta\n",
    "legal = 0\n",
    "fraude = 0\n",
    "\n",
    "#Cuenta de datos\n",
    "for x in range(target.shape[0]):\n",
    "    if target[x] == 0:\n",
    "        legal = legal + 1\n",
    "    else:\n",
    "        fraude = fraude + 1\n",
    "\n",
    "#Representación\n",
    "print(\"Tarjetas legales: \" + str(legal))\n",
    "print(\"Tarjetas fraudulentas: \" + str(fraude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarjetas legales balanceadas: 284315\n",
      "Tarjetas fraudulentas balanceadas: 284315\n"
     ]
    }
   ],
   "source": [
    "#Sobremuestreamos por la gran diferencia\n",
    "\n",
    "#Importamos los paquetes de sobremuestreo\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#SMOTE\n",
    "smote = SMOTE()\n",
    "\n",
    "#Generación de nuevas muestras sintéticas\n",
    "dataSmote, targetSmote = smote.fit_resample(data,target)\n",
    "\n",
    "#Volvemos a contar\n",
    "legal = 0\n",
    "fraude = 0\n",
    "\n",
    "#Cuenta de datos\n",
    "for x in range(targetSmote.shape[0]):\n",
    "    if targetSmote[x] == 0:\n",
    "        legal = legal + 1\n",
    "    else:\n",
    "        fraude = fraude + 1\n",
    "\n",
    "#Representación\n",
    "print(\"Tarjetas legales balanceadas: \" + str(legal))\n",
    "print(\"Tarjetas fraudulentas balanceadas: \" + str(fraude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#División de datos en conjunto de evaluación y conjunto de entrenamiento\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataTrain, dataTest, targetTrain, targetTest = train_test_split(dataSmote,targetSmote, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalización de los datos \n",
    "\n",
    "#Cálculo de la media\n",
    "mean = np.mean(dataTrain, axis=0)\n",
    "\n",
    "#Restamos a las características la media\n",
    "dataTrain -= mean\n",
    "dataTest -= mean\n",
    "\n",
    "#Cálculo de la desviación estándar\n",
    "std = np.std(dataTrain, axis=0)\n",
    "\n",
    "#Dividimos entre la desviación estándar\n",
    "dataTrain /= std\n",
    "dataTest /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento y evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del modelo con Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               7936      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                6425      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 15,037\n",
      "Trainable params: 15,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "#Añadimos las capas de nuestra red neuronal (3 densas y dos de dropout)\n",
    "#Otra opcion seria añadirlas con add (próximo video)\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        #Capa densa, la primera capa siempre tiene que especificar la forma de entrada\n",
    "        keras.layers.Dense(\n",
    "            256, activation=\"relu\", input_shape=(dataTrain.shape[-1],) #Nodos de la capa densa, y función de activación\n",
    "        ),\n",
    "        #keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dense(25, activation=\"relu\"),\n",
    "        #Capa de Dropout. Inactiva algunos de los nodos de la red para evitar el sobreentrenamiento\n",
    "        keras.layers.Dropout(0.3), #El atributo que se pone es el ratio de inactivación\n",
    "        #keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dense(25, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Vemos la forma de nuestro modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones de las métricas (sacadas de internet: https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model)\n",
    "from keras import backend as K\n",
    "\n",
    "#Funciones de las métricas\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21324/21324 [==============================] - 74s 3ms/step - loss: 0.0626 - accuracy: 0.9858 - f1_m: 0.9850 - precision_m: 0.9839 - recall_m: 0.9877 - val_loss: 0.1520 - val_accuracy: 0.9834 - val_f1_m: 0.9822 - val_precision_m: 0.9959 - val_recall_m: 0.9707s: 0.1361 - accuracy: 0.9566 - f1_m: 0.9542 - precision_m: 0.9718 - recall_m: 0.94 - ETA: 51s - loss: 0.1326 - accuracy: 0.9573 - f1_m: 0.9545 - precision_m: 0 - ETA: 47s - loss: 0.1156 - accuracy: 0.9631 - f1_m: 0.9609 - precision_m: 0.9758 - recall_m: 0.95 - ETA: 47s - loss: 0.1183 - accuracy: 0.9629 - f1_m: 0.9606 - precision_m: 0.9757 - recall_m - ETA: 47s - loss: 0.1378 - accuracy: 0.9627 - f1_m: 0.9605 - precision_m: 0.9749 - recall_m - ETA: 46s - loss: 0.1320 - accuracy: 0.9641 - f1_m: 0.9620 - pr - ETA: 44s - loss: 0.1136 - accuracy: 0.9687 - f - ETA: 41s - loss: 0.0981 - accuracy: 0.9722 - f1_m: 0.9706 - precision_m: 0.9781 - recall_m: - ETA: 40s - loss: 0.0963 - accuracy: 0.9727 - f1_m: 0.9712 - precision_m: 0.9785 - recall_m: 0.967 - ETA: 40s - loss: 0.0961 - accuracy: 0.9728 - f1_m: 0.9713 - precision_m: 0.9786 - reca - ETA: 40s - loss: 0.0930 - accuracy: 0.9735 - f1_m: 0.9721 - precision_m: 0.9790 - r - ETA: 39s - loss: 0.0887 - accuracy: 0.9745 - f1_m: 0. - ETA: 36s - loss: 0.0859 - accuracy: 0.9762 - f1_m: 0.9747 - precision_m: 0.9804 - recall_m: - ETA: 36s - loss: 0.0849 - accuracy: 0.9765 - f1_m: 0.9751 - precision_m: 0.9806 - recall_m - ETA: 35s - loss: 0.0838 - accuracy: 0.9768 - f1_m: 0.9754 - precision_m: 0.9806 - recall_m: - ETA: 35s - loss: 0.0828 - accuracy: 0.9770 - f1_m:  - ETA: 33s - loss: 0.0803 - accuracy: 0.9780 - f1_m: 0.9766 - precision_m: 0.9806 - recal - ETA: 33s - loss: 0.0789 - accuracy: 0.9783 - f1_m: 0.9770 - precision_m: 0.9807 -  - ETA: 32s - loss: 0.0791 - accuracy: 0.9787 - f1_m: 0.9774 - precision_m: 0.9808 - recall_m: 0.97 - ETA: 32s - loss: 0.0790 - accuracy: 0.9787 - f1_m: 0.9774 - precision_m: 0.9808 - recall_m: 0. - ETA: 32s - loss: 0.0787 - accuracy: 0.9788 - f1_m: 0.9775 - precision_m: 0 - ETA: 31s - loss: 0.0763 - accuracy: 0.9795 - f1_m: 0.9782 - precis - ETA: 31s - loss: 0.0769 - accuracy: 0.9798 - f1_m: 0.9786 - precision_m: 0.9812 - recall_m:  - ETA: 31s - loss: 0.0768 - accuracy: 0.9798 - f1_m: 0.9786 - precision_m: 0.9812 - recall_m:  - ETA: 30s - loss: 0.0767 - accuracy: 0.9799 - f1_m: 0.9787 - precision_m: 0.9812 - recall_m: 0.9 - ETA: 30s - loss: 0.0768 - accuracy: 0.9799 - f1_m: 0.9787 - ETA: 31s - loss: 0.0755 - accuracy: 0.9802 - f1_m: 0.9791 - precision_m: 0.98 - ETA: 30s - loss: 0.0749 - accuracy: 0.9804 - f1_m: 0.9792 - precision_m: 0.9813 - recall_m - ETA: 30s - loss: 0.0743 - accuracy: 0.9805 - f1_m: 0.9794 - precision_m: 0.9813 - reca - ETA: 30s - loss: 0.0737 - accuracy: 0.9807 - f1_m: 0.9795 - precision_m: 0.9814 - recall_m: 0.97 - ETA: 30s - loss: 0.0736 - accuracy: 0.9807 - f1_m: 0.9795 - precision_m: 0.9814 - r - ETA: 29s - loss: 0.0731 - accuracy: 0.9809 - f1_m: 0.9798 - precision_m: 0.9816 - recall_m: - ETA: 29s - loss: 0.0726 - accuracy: 0.9810 - f1_m: 0.9799 - precision_m: 0.9817 - rec - ETA: 29s - loss: 0.0722 - accuracy: 0.9812 - f1_m: 0.9801 - precision_m: 0.9818 -  - ETA: 28s - loss: 0.0715 - accuracy: 0.9814 - f1_m: 0.9803 - precision_m: 0.9818 - re - ETA: 27s - loss: 0.0709 - accuracy: 0.9815 - f1_m: 0.9804 - precision_m: 0.9819  - ETA: 27s - loss: 0.0699 - accuracy: 0.9817 - f1_m: 0.9806 - precision_m: 0.9820 - recall_m: 0.981 - ETA: 27s - loss: 0.0698 - accuracy: 0.9817 - f1_m: 0.9807 - precision_m: 0.9820 - rec - ETA: 26s - loss: 0.0689 - accuracy: 0.9820 - f1_m: 0.9809 - precision_m: 0.9822 - recall_m: 0.9 - ETA: 26s - loss: 0.0688 - accuracy: 0.9820 - f1_m: 0.9810 - precision_m: 0.9822 - recall_m: 0.98 - ETA: 26s - loss: 0.0688 - accuracy: 0.9820 - f1_m: 0.9810 - precision_m:  - ETA: 25s - loss: 0.0676 - accuracy: 0.9823 - f1_m: 0.9813 - precision_m: 0.9824 - recall_m: 0.982 - ETA: 25s - loss: 0.0676 - accuracy: 0.9 - ETA: 22s - loss: 0.0653 - accuracy: 0.9829 - f1_m: 0.9818 - prec - ETA: 21s - loss: 0.0637 - accuracy: 0.9833 - f1_m: 0.9823 - precision_m: 0.9829 - recall_m: 0 - ETA: 20s - loss: 0.0635 - accuracy: 0.9833 - f1_m: 0.9823 - precision_m: 0.9830 - recall_m: 0.98 - ETA: 20s - loss: 0.0634 - accuracy: 0.9834 - f1_m: 0.9824 - precision_m: 0.9830 - recall_ - ETA: 20s - loss: 0.0629 - accuracy: 0.9835  - ETA: 17s - loss: 0.0613 - accuracy: 0.9839 - f1_m: 0.9829 - precision_m: 0.9833 - ETA: 16s - loss: 0.0654 - accuracy: 0.9840 - f1_m: 0.9830 - precision_m: 0.9833 - recall_m: 0.98 - ETA: 16s - loss: 0.0652 - accuracy: 0.9840 - f1_m: 0.9830 - precision_m: 0.9833 - recall_ - ETA: 16s - loss: 0.0648 - accuracy: 0.9841 - f1_m: 0.9831 - precision_m: 0.9834 - recall_m: 0.984 - ETA: 16s - loss: 0.0648 - accuracy: 0.9841 - f1_m: 0.9831 - precision_m: 0.9834 - re - ETA: 15s - loss: 0.0646 - accuracy: 0.9842 - f1_m: 0.9833 - precision_m: 0.9835 - recall_m: 0.9 - ETA: 15s - loss: 0.0646 - accuracy: 0.9842 - f1_m: 0.9833 - precision_m: 0.9834 -  - ETA: 14s - loss: 0.0640 - accuracy: 0.9843 - f1_m: 0.9834 - precision_m: 0.9835 - recall - ETA: 14s - loss: 0.0635 - accuracy: 0.9844 - f1_m: 0.9835 - precision_m: 0 - ETA: 13s - loss: 0.0666 - accuracy: 0.9843 - f1_m: 0.9834 - precision_m: 0.9833 - recall_m: 0. - ETA: 12s - loss: 0.0668 - accuracy: 0.9843 - f1_m: 0.9834 - precision_m: 0.9833 - recall_m: 0 - ETA: 12s - loss: 0.0667 - accuracy: 0.9843 - f1_m: 0.9834 - precision_m: 0.9832 - recall_m: 0.9 - ETA: 12s - loss: 0.0666 - accuracy: 0.9843 - f1_m: 0.9834 - precision_m: 0.9832 - recall - ETA: 12s - loss: 0.0663 - accuracy: 0.9844 - f1_m: 0.9835 - p - ETA: 10s - loss: 0.0656 - accuracy: 0.9846 - f1_m: 0 - ETA: 9s - loss: 0.0651 - accuracy: 0.9848 - f1_m: 0.9839 - precision_m: 0.9834 -  - ETA: 8s - loss: 0.0651 - accuracy: 0.9848 - f1_m: 0.9840 - precision_m: 0.9835 - recall - ETA: 8s - loss: 0.0649 - accuracy: 0.9849 - f1_m: 0.9841  - ETA: 7s - loss: 0.0643 - accuracy: 0.9850 - f1_m: 0.9842 - precision_m: 0.9836 - reca - ETA: 7s - loss: 0.0642 - accuracy: 0.9850 - f1_m: 0.9842 - precision_m: 0.9835  - ETA: 6s - loss: 0.0639 - accuracy: 0.9851 - f1_m: 0.9842 - precision_m: 0.9836  - ETA: 6s - loss: 0.0636 - accuracy: 0.9851 - f1_m: 0.9843 - pr - ETA: 5s - loss: 0.0629 - accuracy: 0.9852 - f1_m: 0.9844 - pr - ETA: 4s - loss: 0\n",
      "Epoch 2/10\n",
      "21324/21324 [==============================] - 74s 3ms/step - loss: 0.0558 - accuracy: 0.9926 - f1_m: 0.9922 - precision_m: 0.9890 - recall_m: 0.9962 - val_loss: 0.0313 - val_accuracy: 0.9966 - val_f1_m: 0.9964 - val_precision_m: 0.9954 - val_recall_m: 0.9978422 - accuracy: 0.9922 - f1_m: 0.9918 - precision_m: 0.9883 - recall_m: 0.99 - ETA: 59s - loss: 0.0421 - accuracy: 0.992 - ETA: 56s - loss: 0.0443 - accuracy: 0.9922 - f1_m: 0.9919 - precision_m: 0.9883 - r - ETA: 55s - loss: 0.0423 - accuracy: 0.9923 - f1_m: 0.9920 - precision_m - ETA: 54s - loss: 0.0415 - accuracy: 0.9919 - f1_m: 0.9915 - precision_m: 0.9879 - recall_m: 0.996 - ETA: 54s - loss: 0.0414 - accuracy: 0.9919 - f1_m: 0.9915 - precision_m: 0.9879 - recall - ETA: 53s - loss: 0.0404 - accuracy: 0.9920 - f1_m: 0.9916 - precision_m: 0.9880 - recall_m: 0.99 - ETA: 53s - loss: 0.0402 - accuracy: 0.9919 - f1_m: 0.9915 - precision_m: 0.9879 - recall_m:  - ETA: 53s - loss: 0.0396 - accuracy: 0.9920 - f1_m: 0.9916 - precision_m: 0.9880 - rec - ETA: 53s - loss: 0.0407 - accuracy: 0.9920 - f1_m: 0.9916 - precision_m: 0.9879 - rec - ETA: 52s - loss: 0.0398 - accuracy: 0.9921 - f1_m: 0.9917 - precision_m: 0.9880 - rec - ETA: 52s - loss: 0.0392 - accuracy: 0.9921 - f1_m: 0.9918 - precision_m: 0.9881 - reca - ETA: 51s - loss: 0.0381 - accuracy: 0.9923 - f1_m: 0.9919 - precision_m: 0.9883 - - ETA: 50s - loss: 0.0373 - accuracy: 0.9924 - f1_m: 0.9920 - precision_m: 0.9884 - recall_m - ETA: 50s - loss: 0.0367 - accuracy: 0.9924 - f1_m: 0.9921 - precision_m: 0.9884 - recall_ - ETA: 49s - loss: 0.0360 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9886 - recall_m:  - ETA: 49s - loss: 0.0367 - accuracy: 0.9923 - f1_m: 0.9920 - precision_m: 0.98 - ETA: 48s - loss: 0.0380 - accuracy: 0.9922 -  - ETA: 45s - loss: 0.0359 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9888 - recall_m: 0.996 - ETA: 45s - loss: 0.0358 - accuracy: 0.9925 - f1_m: 0.99 - ETA: 43s - loss: 0.0344 - accuracy: 0.9927 - f1_m: 0.9923 - precision_m: 0.9891 - recal - ETA: 42s - loss: 0.0342 - accuracy: 0.9927 - f1_m: 0.9923 - precision_m: 0.9890 - recall_m: 0 - ETA: 42s - loss: 0.0341 - accuracy: 0.9927 - f1_m: 0.9923 - precision_m: 0.989 - ETA: 41s - loss: 0.0421 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m:  - ETA: 40s - loss: 0.0421 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.9889 - recall_m: 0. - ETA: 40s - loss: 0.0423 - a - ETA: 36s - loss: 0.0416 - accuracy: 0.9924 - f1_m: 0.9920 - precision_m: 0.9888 - re - ETA: 35s - loss: 0.0415 - accuracy: 0.9924 - f1_m: 0.9920 - precision_m: 0.9887 - recall_m: 0.9 - ETA: 35s - loss: 0.0414 - accuracy: 0.9924 - f1_m: 0.9920 - precision_m: 0.9888 - r - ETA: 34s - loss: 0.0452 - accuracy: 0.9924 - f1_m: 0.9920 - precision_m: 0.9888 - recall_m:  - ETA: 34s - loss: 0.0449 - accuracy: 0.9924 - f1_m: 0.9920 - precision_m: 0.9888 - recal - ETA: 33s - loss: 0.0444 - accuracy: 0.9925 - f - ETA: 31s - loss: 0.0444 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.9890 - recall_m: 0 - ETA: 30s - loss: 0.0444 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.9889 - recall_ - ETA: 30s - loss: 0.0443 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.9889 - recall_m: - ETA: 30s - loss: 0.0441 - accur - ETA: 26s - loss: 0.0505 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0. - ETA: 25s - loss: 0.0499 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.9889 - recall_m - ETA: 24s - loss: 0.0495 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.9890 - recall_m: 0 - ETA: 24s - loss: 0.0495 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.9889 - ETA: 23s - loss: 0.0490 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.9890 - ETA: 22s - loss: 0.0483 - accuracy: 0.9926 - f1_m: 0.9922 - precision_m: 0.9890 - recall_m: 0. - ETA: 22s - loss: 0.0482 - accuracy: 0.9926 - f1_m: 0.9922 - precision_m: 0.9890 - recall_m: 0. - ETA: 22s - loss: 0.0480 - accuracy: 0.9926 - f1_m: 0.9922 - precision_m: 0.9891 - rec - ETA: 21s - loss: 0.0476 - accuracy: 0.9926 - f1_m: 0.9923 - precision_m: 0.9891 - recall_m: - ETA: 21s - loss: 0.0473 - accuracy: 0.9927 - f1_m: 0.9923 - precision_m: 0.9892 - reca - ETA: 20s - loss: 0.0468 - accuracy: 0.9927 - f1_m: 0.9923 - precision_m: 0.9892  - ETA: 19s - loss: 0.0468 - accuracy: 0.9927 - f1_m: 0.9923 - precision_m: 0.9893 - recall_m: 0. - ETA: 19s - loss: 0.0467 - accuracy: 0.9927 - f1_m: 0.9923 - precision_m: 0.9893 - - ETA: 18s - loss: 0.0471 - accuracy: 0.9927 - f1_m: 0.9923 - precision_m: 0.9893 - recall_m: 0.9 - ETA: 18s - loss: 0.0471 - accuracy: 0.9927 - f1_m: 0.9923 - precision_m: 0.9893 - reca - ETA: 17s - loss: 0.0470 - accuracy: 0.9927 - f1_m: 0.9923 - precision_m: 0.9893 - recall_m - ETA: 17s - loss: 0.0468 - accuracy: 0.9927 - f1_m: 0.9923 - precision_m: 0.9892  - ETA: 16s - loss: 0.0467 - accuracy: 0.9927 - f1_m: 0.9923 - precision_m: 0.9892 - recall_m: 0 - ETA: 16s - loss: 0.0467 - accuracy: 0.9927 - f1_m: 0.9923 - precision_m: 0.9893 -  - ETA: 15s - loss: 0.0473 - accuracy: 0.9926 - f1_m: 0.9922 - precision_m: 0.9892 -  - ETA: 14s - loss: 0.0474 - accuracy: 0.9926 - f1_m: 0.9922 - precision_m: 0.9891 - recall_m: 0. - ETA: 14s - loss: 0.0473 - accuracy: 0.9926 - f1_m: 0.9922 - precision_m: 0.9891 - recall_m: - ETA: 14s - loss: 0.0472 - accuracy: 0.9926 - f1_m: 0.9922 - precision_m: 0.9891 - recall_m:  - ETA: 13s - loss: 0.0470 - accuracy: 0.9926 - f1_m: 0.9922 - precision_m: 0.9891 -  - ETA: 12s - loss: 0.0476 - accuracy: 0.9926 - f1_m: 0.9922 - precision_m: 0.9891 - recall_m: 0.99 - ETA: 12s - loss: 0.0478 - accuracy: 0.9926 - f1_m: 0.9922 - precision_m: 0.9891 - recall_m: 0.9 - ETA: 12s - loss: 0.0477 - accuracy: 0.9926 - f1_m: 0.9922 - preci - ETA: 10s - loss: 0.0528 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.9890 - recall_m: 0 - ETA: 10s - loss: 0.0527 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m:  - ETA: 9s - loss: 0.0522 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.9890 - recall_m - ETA: 9s - loss: 0.0521 - accuracy: 0.9926 - f1_m: 0.9921 - precision_m: 0.9890 - recall_m: 0. - ETA: 9s - loss: 0.0520 - accuracy: 0.9926 - f1_ - ETA: 8s - loss: 0.0588 - accuracy: 0.9925 - f1_m: 0.9921 - precision_ - ETA: 4s - loss: 0.0586 - accuracy: 0.9924 - f - ETA: 3s - loss: 0.0581 - ac - ETA: 1s - loss: 0.0569 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.9890 -  - ETA: 1s - loss: 0.0567 - accuracy: 0.9926 - f1_m: 0.9922 - precision_m: 0.9890 - re - ETA: 1s - loss: 0.0565 - accuracy: 0.9926 - f1_m: 0.9921 - precision_m: 0.9890 - recall_m: 0.99 - ETA: 1s - loss: 0.0565 - accuracy: 0.9926 - f1_m: 0.9921 - precision_m: 0.9889 - recall_m: 0. - ETA: 0s - loss: 0.0564 - accuracy: 0.9926 - f1_m: 0.9921 - precision_m: 0.9889 - recall_m:  - ETA: 0s - loss: 0.0563 - accuracy: 0.9926 - f1_m: 0.9921 - precisio\n",
      "Epoch 3/10\n",
      "21324/21324 [==============================] - 75s 3ms/step - loss: 0.0405 - accuracy: 0.9926 - f1_m: 0.9923 - precision_m: 0.9880 - recall_m: 0.9974 - val_loss: 0.0201 - val_accuracy: 0.9961 - val_f1_m: 0.9958 - val_precision_m: 0.9928 - val_recall_m: 0.99930.9881 -  - ETA: 1:00 - loss: 0.0307 - accuracy: 0.9930 - f1_m: 0.9926 - precision_m:  - ETA: 59s - loss: 0.0310 - accuracy: 0.9929 - f1_m: 0.9926 - precision_m: 0.9887 - recall_m: 0. - ETA: 59s - loss: 0.0329 - accuracy: 0.9924 - f1_m: 0.9921 - precision_m: 0.9878 - recall_m: 0.99 - ETA: 59s - loss: 0.0331 - accuracy: 0.9924 - f1_m: 0.9920 - precision_m: 0.9877 - r - ETA: 58s - loss: 0.0342 - accuracy: 0.9915 - f1_m: 0.9911 - precision_m: 0.9862 - recall_m: 0 - ETA: 58s - loss: 0.0334 - accuracy: 0.9916 - f1_m: 0.9912  - ETA: 55s - loss: 0.0378 - accuracy: 0.9920 - f1_m: 0.9916 - precision_m: 0.9872 - reca - ETA: 55s - loss: 0.0373 - accuracy: 0.9919 - f1_m: 0.9916 - precision_m:  - ETA: 54s - loss: 0.0366 - accuracy: 0.9919 - f1_m: 0.9916 - precision_m: 0.9871 - recall_m: 0.997 - ETA: 54s - loss: 0.0365 - accuracy: 0.9920 - f1_m: 0.9916 - precision_m: 0.9871 - reca - ETA: 53s - loss: 0.0370 - accuracy: 0.9917 - f1_m: 0.9914 - precision_m: 0.9868 - re - ETA: 53s - loss: 0.0369 - accuracy: 0.9917 - f1_m: 0.9912  - ETA: 50s - loss: 0.0351 - accuracy: 0.9919 - f1_m: 0.9916 - precision_m: 0.9873 - recall_m: - ETA: 50s - loss: 0.0347 - accuracy: 0.9920 - - ETA: 47s - loss: 0.0333 - accuracy: 0.9921 - f1_m: 0.9918 - precisio - ETA: 46s - loss: 0.0325 - accuracy: 0.9922 - f1_m: 0.9919 - precision_m: 0.9877 - recall_m: 0. - ETA: 46s - loss: 0.0322 - accuracy: 0.9923 - f1_m: 0.9919 - precision_m: - ETA: 44s - loss: 0.0327 - accuracy: 0.9922 - f1_m: 0.9918 - precision_m: 0.9876 - recall_m - ETA: 44s - loss: 0.0330 - accuracy: 0.9920 - f1_m: 0.9917 - precision_m: 0.9874 - recall_m: 0 - ETA: 43s - loss: 0.0329 - accuracy: 0.9920 - f1_m: 0.9917 - precision_m: 0.9874 -  - ETA: 43s - loss: 0.0323 - accuracy: 0.9921 - f1_m: 0.9918 - precision_m: 0.9876 - reca - ETA: 42s - loss: 0.0318 - accuracy: 0.9922 - f1_m: 0.9919 - precision_m: 0.9876 - r - ETA: 41s - loss: 0.0368 - accuracy: 0.9922 - f1_m: 0.9918 - precision_m: 0.9876 - recall_m - ETA: 41s - loss: 0.0366 - accuracy: 0.9922 - f1_m: 0.9918 - precision_m: 0.9875 - recall_m: 0.99 - ETA: 41s - loss: 0.0364 - accuracy: 0.9922 - f1_m: 0.9918 - precision_m: 0.9876 - recall_m: 0.997 - ETA: 41s - loss: 0.0364 - accuracy: 0.9922 - f1_m: 0.9919 - precision_m: 0.9876 - - ETA: 40s - loss: 0.0401 - accuracy: 0.9922 - f1_m: 0.9919 - precision_m: 0.9877 - recall_m: 0.9 - ETA: 40s - loss: 0.0401 - accuracy: 0.9922 - f1_m: 0.9918 - precision_m: 0.9876 - recall_m: 0 - ETA: 40s - loss: 0.0398 - accuracy: 0.9922 - f1_m: 0.9919 - precision_m: 0.9877 - r - ETA: 39s - loss: 0.0393 - accuracy: 0.9923 - f1_m: 0.9919 - precision_m: 0.9877 - recall_m: 0 - ETA: 39s - loss: 0.0391 - accuracy: 0.9923 - f1_m: 0.9920 - precisi - ETA: 37s - loss: 0.0430 - accuracy: 0.9924 - f1_m: 0.9921 - precision_m: 0.9881 - recall_m - ETA: 37s - loss: 0.0430 - accuracy: 0.9924 - f1_m:  - ETA: 34s - loss: 0.0423 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9882 - recall_m: 0. - ETA: 34s - loss: 0.0422 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9882 - recall_m: 0.997 - ETA: 34s - loss: 0.0422 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9882 - recall - ETA: 33s - loss: 0.0421 - accuracy: 0.9924 - f1_m: 0.9921 - precis - ETA: 32s - loss: 0.0410 - accuracy: 0.9926 - f1_m: 0.9923 - precision_m: 0.9883 - recall_m: 0 - ETA: 32s - loss: 0.0410 - accuracy: 0.9926 - f1_m: 0.9923 - precision_ - ETA: 30s - loss: 0.0402 - accuracy: 0.9927 - f1_m: 0.9924 - precision_m: 0.9884 - recall_m: 0 - ETA: 30s - loss: 0.0400 - accuracy: 0.9927 - f1_m: 0.9924 - precision_m: 0.9885 - recall_m: 0. - ETA: 30s - loss: 0.0400 - accuracy: 0.9927 - f1_m: 0.9924 - precision_m: 0.9885 - re - ETA: 29s - loss: 0.0401 - accuracy: 0.9927 - f1_m: 0.9924 - precision_m: 0.9884 - re - ETA: 28s - loss: 0.0399 - accuracy: 0.9926 - f1_m: 0.9923 - precision_m - ETA: 27s - loss: 0.0395 - accuracy: 0.9926 - f1_m: 0.9923 - ETA: 25s - loss: 0.0384 - accuracy: 0.9928 - f1_m: 0.9925 - precision_m:  - ETA: 23s - loss: 0.0379 - accuracy: 0.9928 - f1_m: 0.9925 - precision_m: 0.9886 - recall_m - ETA: 23s - loss: 0.0380 - accuracy: 0.9928 - f1_m: 0.9925 - precision_m: 0.9886 - recall_m - ETA: 18s - loss: 0.0421 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9880 - recall_m: 0.9 - ETA: 18s - loss: 0.0421 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9880 - recall_m: - ETA: 17s - loss: 0.0419 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9880 - recall_m: - ETA: 17s - loss: 0.0418 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9880 - rec - ETA: 16s - loss: 0.0416 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m - ETA: 15s - loss: 0.0412 - accuracy: 0.9925 - f1_m: 0.9922  - ETA: 13s - loss: 0.0405 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m - ETA: 12s - loss: 0.0401 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.9 - ETA: 11s - loss: 0.0398 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9879 - recall_m: 0.99 - ETA: 10s - loss: 0.0398 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9879 - recall - ETA: 10s - loss: 0.0400 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9879  - ETA: 9s - loss: 0.0399 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.987 - ETA: 9s - loss: 0.0399 - accuracy: 0.9925 - f1_m: 0.9921 - precision_m: 0.9878 - recall_m:  - - ETA: 6s - loss: 0.0409 - accuracy: 0.9924 - f1_m: 0.9921 - precision_m: 0.9878 - recall - ETA: 6s - loss: 0.0408 - accuracy: 0.9924 - f1_m: 0.9921 -  - ETA: 5s - loss: 0.0404 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9879  - ETA: 4s - loss: 0.0416 - accuracy: 0.9925 - f1_m: 0.9922 - precision_m: 0.9879 - recall - ETA: 4s - loss: 0.0416 - accuracy: 0.9925 - f1_m: 0.9922  - ETA: 3s - loss: 0.0412 - accuracy: 0.9926 - f1_m: 0 - ETA: 2s - loss: 0.0407 - accuracy: 0.9926 - f1_m: 0.9923 - precision_m: 0.9880 - recall - ETA: 1s - loss: 0.0407 - accuracy: 0.9926 - f1_m: 0.9923 - precision_m: 0.9881 - re - ETA: 1s - loss: 0.0405 - accuracy: 0.9926 - - ETA: 0s - loss: 0.0405 - accuracy: 0.9926 - f1_m: 0.9923 - precision_m: 0.9880 - recall_m: 0.99\n",
      "Epoch 4/10\n",
      "21324/21324 [==============================] - 79s 4ms/step - loss: 0.0597 - accuracy: 0.9931 - f1_m: 0.9927 - precision_m: 0.9882 - recall_m: 0.9979 - val_loss: 0.0083 - val_accuracy: 0.9979 - val_f1_m: 0.9978 - val_precision_m: 0.9960 - val_recall_m: 0.99983 - f1_m: 0.9941 - precision_m: 0.9910 -  - ETA: 1:00 - loss: 0.0308 - accuracy: 0.9937 - f1_m: 0.9934 - precision_m: 0 - ETA: 59s - loss: 0.0435 - accuracy: 0.9931 - f1_m: 0.9927 - precision_m: 0.9889 - recall_m: - ETA: 59s - loss: 0.0401 - accuracy: 0.9935 - f1_m: 0.9930 - precision_m - ETA: 58s - loss: 0.0353 - accuracy: 0.9938 - f1_m: 0.9935 - precision_m: 0.9899 - ETA: 57s - loss: 0.0319 - accuracy: 0.9943 - f1_m: 0.9941  - ETA: 56s - loss: 0.0293 - accuracy: 0.9944 - f1_m: 0.9942 - precision_m: 0.9909 - recall_m: 0.998 - ETA: 56s - loss: 0.0293 - accuracy: 0.9945 - f1_m: 0.9942 -  - ETA: 55s - loss: 0.0290 - accuracy: 0.9944 - f1_m: 0.9941 - precision_m: 0.9906  - ETA: 55s - loss: 0.0581 - accuracy: 0.9940 - f1_m: 0.9938 - precision_m: 0.9902 - r - ETA: 55s - loss: 0.0566 - accuracy: 0.9939 - - ETA: 54s - loss: 0.0513 - accuracy: 0.9939 - f1_m: 0.9936 - precision_m: 0.9898 - recall_m - ETA: 54s - loss: 0.0504 - accuracy: 0.9939 - f1_m: 0.9936 - precision_m: 0.98 - ETA: 53s - loss: 0.0482 - accuracy: 0.9940 - f1_m: 0.9936 - precision_m: 0.9899 - recall - ETA: 53s - loss: 0.0474 - accuracy: 0.9940 - f1_m: 0.9936 - prec - ETA: 51s - loss: 0.0508 - accuracy: 0.9937 - f1_m: 0.9934 - precision_m: 0.9896 - recall_m:  - ETA: 51s - loss: 0.0504 - accuracy: 0.9937 - f1_m: 0.9934 - precision_m: 0.9896 - recall_m: 0.99 - ETA: 51s - loss: 0.0560 - accuracy: 0.9937 - f1_m: 0.9933 - precision_m: 0.9896 - - ETA: 50s - loss: 0.0544 - accuracy: 0.9936 - f1_m: 0.9933 - pr - ETA: 49s - loss: 0.0519 - accuracy: 0.9937 - f1_m: 0.9933 - precision_m: 0.9894 - recall_m: 0.9 - ETA: 48s - loss: 0.0517 - accuracy: 0.9937 - f1_m: 0.9933 - precision_m: 0.9894 - recall_m - ETA: 48s - loss: 0.0511 - accuracy: 0.9937 - f1_m: 0.9933 - precision_m: 0.9894 - recall_m: 0.99 - ETA: 48s - loss: 0.0509 - accuracy: 0.9937 - f1_m: 0.9933 - precision_m: 0.9894 - recall_m: 0.998 - ETA: 48s - loss: 0.0509 - accuracy: 0.9937 - f1_m: 0.9933 - precision_m: 0 - ETA: 47s - loss: 0.0493 - accuracy: 0.9938 - f1_m: 0.9934 - precision_m: 0.9896 - recall_m: 0.998 - ETA: 47s - loss: 0.0492 - accuracy: 0.9938 - f1_m: 0.9934 - precision_m: 0.9896 - recall_m: 0 - ETA: 46s - loss: 0.0499 - accuracy: 0.9938 - f1_m: 0.9934 - precision_m: 0.9896 - recall_m: 0.9 - ETA: 46s - loss: 0.0497 - accuracy: 0.9938 - f1_m: 0.9934 - precision_m: 0.9896 - recall_m - ETA: 46s - loss: 0.0491 - accuracy: 0.9938 - f1_m: 0.9934 - precision_m: 0.9896 - recall_m: 0. - ETA: 46s - loss: 0.0490 - accuracy: 0.9938 - f1_m: 0.9934 - precision_m: 0.9896 - recall_m: 0 - ETA: 45s - loss: 0.0496 - accuracy: 0.9937 - f1_m: 0.9933 - precision_m: 0.9894 - recall_m: 0.99 - ETA: 45s - loss: 0.0495 - accuracy: 0.9937 - f1_m: 0.9933 - precision_m: 0.9893 - recall_m - ETA: 45s - loss: 0.0494 - accuracy: 0.9937 - f1_m: 0.9933 - precision_m: 0.9893 - recall_m: 0.998 - ETA: 45s - loss: 0.0493 - accuracy: 0.9937 - f1_m: 0.9933 - precision_m: 0.9893 - recal - ETA: 44s - loss: 0.0489 - accuracy: 0.9935 - f1_m: 0.9931 - precision_m: 0.9891 - recall - ETA: 44s - loss: 0.0484 - accuracy: 0.9935 - f1_m: 0.9931 - precision_m: 0.9890 - recall_m:  - ETA: 43s - loss: 0.0480 - accuracy: 0.9935 - f1_m: 0.9931 - precision_m: 0.9890 - recall_m: 0.99 - ETA: 43s - loss: 0.0479 - accuracy: 0.9935 - f1_m: 0.9931 - precisi - ETA: 42s - loss: 0.0462 - accuracy: 0.9936 - f1_m: 0.9932 - precision_m: 0.9892 - ETA: 41s - loss: 0.0617 - accuracy: 0.9936 - f1_m: 0.9932 - precision_m: 0.9893 - recall_m:  - ETA: 41s - loss: 0.0615 - accuracy: 0.9936 - f1_m: 0.9932 - precis - ETA: 39s - loss: 0.0605 - accuracy: 0.9935 - f1_m: 0.9931 - precision_m: 0.9891 - recall_m: 0.9 - ETA: 39s - loss: 0.0604 - accuracy: 0.9934 - f1_m: 0.9930 - precision_m: 0.9890 - recall_m: 0.9 - ETA: 39s - loss: 0.0603 - accuracy: 0.9934 - f1_m: 0.9930 - precision_m - ETA: 37s - loss: 0.0587 - accuracy: 0.9935 - f1_m: 0.9931 - precision_m: 0.9891 - recall_m: 0. - ETA: 37s - loss: 0.0585 - accuracy: 0.9935 - f1_ - ETA: 35s - loss: 0.0564 - accuracy: 0.9933 - f1_m: 0.9929 - precision_m: 0.9887 - recall - ETA: 34s - loss: 0.0562 - accuracy: 0.9933 - f1_m: 0.9929 - precision_m: 0.9886 - recall_ - ETA: 34s - loss: 0.0562 - accuracy: 0.9933 - f1_m: 0.9928 - precision_m: 0.9886 - reca - ETA: 33s - loss: 0.0557 - accuracy: 0.9933 - f1_m: 0.9928 - precision_m: 0.9886 - recall - ETA: 33s - loss: 0.0553 - accuracy: 0.9933 - f1_m: 0.9929 - precision_m: 0.9886 - recall_m: - ETA: 32s - loss: 0.0550 - accuracy: 0.9933 - f1_m: 0.9929 - precision_m: 0.9887 - r - ETA: 32s - loss: 0.0545 - accuracy: 0.9933 - f1_m: 0.9929 - precision_m: 0.9886 - recall_m: 0.998 - ETA: 32s - loss: 0.0545 - accuracy: 0.9933 - f1_m: 0.9929 - precision_m: 0.9886 - re - ETA: 31s - loss: 0.0539 - accuracy: 0.9933 - f1_m: 0.9929 -  - ETA: 29s - loss: 0.0531 - accuracy: 0.9933 - f1_m: 0.9929 - precision_m: 0.9886 - recall_m:  - ETA: 29s - loss: 0.0529 - accuracy: 0.9933 - f1_m: 0.9929 - precision_m: 0.9886 - reca - ETA: 28s - loss: 0.0529 - accuracy: 0.9932 - f1_m: 0.9928 - precision_m: 0.9885 - recall_m: - ETA: 28s - loss: 0.0531 - accuracy: 0.9932 - f1_m: 0.9928 -  - ETA: 26s - loss: 0.0522 - accuracy: 0.9931 - f1_m: 0.9927 - precision_m: 0.9882 - recall_m: 0 - ETA: 25s - loss: 0.0520 - accuracy: 0.9931 - f1_m: 0.9927 - precision_m: 0.9883 - recall_m: 0. - ETA: 25s - loss: 0.0519 - accuracy: 0.9931 - f1_m: 0.9927 - precision_m: 0.9883 - recall_m: 0.997 - ETA: 25s - loss: 0.0518 - accuracy: 0.9931 - f1_m: 0.9927 - precision_m: 0.9 - ETA: 24s - loss: 0.0522 - accuracy: 0.9930 - f1_m: 0.9925 - precision_m: 0.9881 - recall_m:  - ETA: 23s - loss: 0.0520 - accuracy: 0.9929 - f1_m: 0 - ETA: 21s - loss: 0.0506 - accuracy: 0.9928 - f1_m: 0.9924 - precision_m: 0.9878 - recall_m: 0.9 - ETA: 20s - loss: 0.0505 - accuracy: 0.9928 - f1_m: 0.9924 - precision_m: 0.9878 - recall_m: 0 - ETA: 20s - loss: 0.0503 - accuracy: 0.9929 - f1_m: 0.9925 - precision_m: 0.9878 - ETA: 19s - loss: 0.0498 - accuracy: 0.9929 - f1_m: 0.9925 - precision_m: 0.9880 - recall_m:  - ETA: 19s - loss: 0.0496 - accuracy: 0.9930 - f1_m: 0.9926 - precision_m: 0.9880 - - ETA: 18s - loss: 0.0492 - accuracy: 0.9930 - f1_m: 0.9926 - precision_m: 0 - ETA: 16s - loss: 0.0485 - accuracy: 0.9931 - f1_m: 0.9927 - precision_m: 0.9882  - ETA: 15s - loss: 0.0483 - accuracy: 0.9930 - f1_m: 0.9925 - precision_m: 0.9880 - recall_m: 0.9 - ETA: 15s - loss: 0.0482 - accuracy: 0.9930 - f1_m: 0.9925 - precision_m: 0.9 - ETA: 14s - loss: 0.0476 - accuracy: 0.9930 - f1_m: 0.9925 - precision_m: 0.9880 - recall_m: 0.998 - ETA: 14s - loss: 0.0476 - accuracy: 0.9930 - f1_m: 0.9925 - precision_m: 0.9880 - recall_m - ETA: 13s - loss: 0.0473 - accuracy: 0.9930 - f1_m: 0.9925 - precision_m: 0.9880 - re - ETA: 12s - loss: 0.0507 - accuracy: 0.9930 - f1_m: 0.9926 - precision_m: 0.9880  - ETA: 11s - loss: 0.0504 - accuracy: 0.9930 - f1_m: 0.9925 - precision_m: 0.9880 - - ETA: 10s - loss: 0.0506 - accuracy: 0.9930 - f1_m: 0.9925 - precision_m: 0.9880 - recall_ - ETA: 10s - loss: 0.0505 - acc - ETA: 8s - loss: 0.0494 - accuracy: 0.9930 - f1_m: 0.9926 - precision_m: 0.9880 - reca - ETA: 7s - loss: 0.0494 - accuracy: 0.9930 - f1_m: 0.9926 - precision_ - ETA: 7s - loss: 0.0491 - accuracy: 0.9930 - f1_m: 0.9925 - precision_m: 0.9880 - recall_m: 0. - ETA: 6s - loss: 0.0491 - accuracy: 0.9930 - f1_m: 0.9925 - precisio - ETA: 6s - loss: 0.0488 - accuracy: 0.9930 - f1_m: 0.9925 - precision_m: 0.9880 - recall_m: 0.99 - ETA: 6s - loss: 0.0488 - accuracy: 0.9930 - f1_m: 0.9925 -  - ETA: 4s - loss: 0.0483 - accuracy: 0.9930 - f1_m: 0.9925  - ETA: 3s - loss: 0.0479 - accu - ETA: 1s - loss: 0.0606 - accuracy: 0.9930 - f1_m: 0.9926 - precision_m: 0.9882 - recall_m: 0. - ETA: 1s - loss: 0.0605 - accuracy: 0.9930 - f1_m: 0.9926 - precision_m: 0 - ETA: 1s - loss: 0.0602 - accuracy: 0.9930 - f1_m: 0.9926 - precis - ETA: 0s - loss: 0.0599 - accuracy: 0.9930 - f1_m: 0.9926 - precision_m: 0.9882 - reca\n",
      "Epoch 5/10\n",
      "21324/21324 [==============================] - 74s 3ms/step - loss: 0.0481 - accuracy: 0.9915 - f1_m: 0.9911 - precision_m: 0.9852 - recall_m: 0.9982 - val_loss: 0.0141 - val_accuracy: 0.9975 - val_f1_m: 0.9973 - val_precision_m: 0.9961 - val_recall_m: 0.9989: 0.0156 - accuracy: 0.9958 - f1_m: 0.9952 - precision_m: 0.9922 - reca - ETA: 1:02 - loss: 0.0263 - accuracy: 0.9944 - f1_m: 0.9940 - precision_m: 0 - ETA: 1:02 - loss: 0.0285 - accuracy: 0.9936 - f1_m: 0.9931 - precision_m: 0.989 - ETA: 1:01 - loss: 0.0314 - accuracy: 0.9931  - ETA: 59s - loss: 0.0300 - accuracy: 0.9931 - f1_m: 0.9927 - precision_m: 0.9882 -  - ETA: 58s - loss: 0.0308 - accuracy: 0.9931 - f1_m: 0.9926 - precision_m: 0.9883 - recal - ETA: 58s - loss: 0.0288 - accuracy: 0.9934 - f1_m: 0.9930 - prec - ETA: 56s - loss: 0.0257 - accuracy: 0.9940 - f1_m: 0.9937 - precision_m: 0.9899 - recall_m: - ETA: 56s - loss: 0.0261 - accuracy: 0.9938 - f1_m: 0.9935 - precision_m: 0.9897 - recall_m: 0.99 - ETA: 55s - loss: 0.0261 - accuracy: 0.9938 - f1_m: 0.9934 - precision_m: 0.98 - ETA: 54s - loss: 0.0251 - accuracy: 0.9941 - f1_m: 0.9938 - precision_m: 0.9901 - re - ETA: 54s - loss: 0.0243 - accuracy: 0.9943 - f1_m: 0.9940 - precision_m: 0.9903  - ETA: 53s - loss: 0.0251 - accuracy: 0.9942 - f1_m: 0.9939 - precision_m: 0.9901 - recall - ETA: 52s - loss: 0.0252 - accuracy: 0.9942 - f1_m: 0.9938 - precision_m: 0.9900 - recall_m: 0.998 - ETA: 52s - loss: 0.0252 - accuracy: 0.9942 - f1_m: 0.9939 - precision_m: 0.9900 - reca - ETA: 52s - loss: 0.0249 - accuracy: 0.9942 - f1_m: 0.9939 - precision_m:  - ETA: 51s - loss: 0.0247 - accuracy: 0.9942 - f1_m: 0.9939 - precision_m: 0.9900 - recall_m: 0 - ETA: 50s - loss: 0.0246 - accuracy: 0.9942 - f1_m: 0.9939 - precision_m: 0.9900 - recal - ETA: 50s - loss: 0.0259 - accuracy: 0.9941 - f1_m: 0.9937 - precision_m: 0.9898 - r - ETA: 49s - loss: 0.0267 - accuracy: 0.9939 - f1_m: 0.9936 - precision_m: 0.9896 - recall_ - ETA: 48s - loss: 0.0264 - accuracy: 0.9940 - f1_m: 0.9937 - precision_m: 0.9897 - recall_m - ETA: 48s - loss: 0.0263 - accuracy: 0.9940 - f1_m: 0.9937 - precision_m: 0.9897 - recall_m: 0.998 - ETA: 48s - loss: 0.0263 - accuracy: 0.9940 - f1_m: 0.9937 - precision_m: 0.9898 - reca - ETA: 47s - loss: 0.0261 - accuracy: 0.9940 - f1_m: 0.9937 - precision_m: 0.989 - ETA: 46s - loss: 0.0271 - accuracy: 0.9938 - f1_m: 0.9935 - precision_m: 0.9895 - recall_m: 0.9 - ETA: 46s - loss: 0.0272 - accuracy: 0.9938 - f1_m: 0.9935 - precision_m - ETA: 45s - loss: 0.0269 - accuracy: 0.9936 - f1_m: 0.9932 - precision_m: 0.9891 - recall_m: 0. - ETA: 44s - loss: 0.0268 - accuracy: 0.9936 - f1_m: 0.9933 - preci - ETA: 43s - loss: 0.0305 - accuracy: 0.9934 - f1_m: 0.9931 - precision_m: 0.9889 - recall_m: - ETA: 43s - loss: 0.0305 - accuracy: 0.9934 - f1_m: 0.9931 - precision_m: 0.9889 - recall_m: 0 - ETA: 42s - loss: 0.0304 - accuracy: 0.9934 - f1_m: 0.9931 - precision_m: 0.9889 - recall_m: - ETA: 42s - loss: 0.0301 - accuracy: 0.9934 - f1_m: 0.9931 - precision - ETA: 40s - loss: 0.0298 - accuracy: 0.9934 - f1_m: 0.9931 - precision_m: 0.9888 - - ETA: 40s - loss: 0.0306 - accuracy: 0.9932 - f1_m: 0.9928 -  - ETA: 38s - loss: 0.0316 - accuracy: 0.9929 - f1_m: 0.9925 - precision_m: 0.9877 - recall_m: 0. - ETA: 37s - loss: 0.0315 - accuracy: 0.9929 - f1_m: 0.9925 - precision_m:  - ETA: 36s - loss: 0.0326 - accuracy: 0.9929 - f1_m: 0.9924 - precision_m: 0.9878 - recall_m: 0 - ETA: 36s - loss: 0.0325 - accuracy: 0.9928 - f1_m: 0.9924 - precision_m: 0.9878 - re - ETA: 35s - loss: 0.0529 - accuracy: 0.9925 - f1_m: 0.9920 - precision_m: 0.9872 - recall - ETA: 35s - loss: 0.0529 - accuracy: 0.9924 - f1_m: 0.9919 - precision_m: 0.9870 - rec - ETA: 34s - loss: 0.0528 - accuracy: 0.9923 - f1_m: 0.9918 - precision_m: 0.9869 - recall_m: 0.99 - ETA: 34s - loss: 0.0527 - accuracy: 0.9922 - f1_m: 0.9918 - precision_m: 0.9868 -  - ETA: 34s - loss: 0.0526 - accuracy: 0.9922 - f1_m: 0.9917 - precision_m:  - ETA: 33s - loss: 0.0517 - accuracy: 0.9922 - f1_m: 0.9917 - precision_m: 0.9867 - recal - ETA: 32s - loss: 0.0513 - accuracy: 0.9922 - f1_m: 0.9917 - precision_m: 0.9867 - recall_m:  - ETA: 32s - loss: 0.0519 - accuracy: 0.9921 - f1_m: 0.9917 - precisio - ETA: 30s - loss: 0.0513 - accuracy: 0.9920 - f1_m: 0.9915 - precision_m: 0.9863 - recall_m: 0 - ETA: 30s - loss: 0.0511 - accuracy: 0.9920 - f1_m: 0.9915 - precision_m - ETA: 29s - loss: 0.0499 - accuracy: 0.9919 - f1_m: 0.9915 - precision_m: 0.9862 - recall_m:  - ETA: 28s - loss: 0.0499 - accuracy: 0.9920 - f1_m: 0.9915 - precision_m: 0.9862 - recall - ETA: 28s - loss: 0.0496 - accuracy: 0.9919 - f1_m: 0.9915 - precision_m: 0.9862 - recall_ - ETA: 27s - loss: 0.0492 - accuracy: 0.9919 - f1_m: 0.9915 - precision_m: 0.9862 - recall_m: 0.9 - ETA: 27s - loss: 0.0492 - accuracy: 0.9920 - f1_m: 0.9915 - precision_m: 0.9862 - recall_m:  - ETA: 27s - loss: 0.0490 - accuracy: 0.9920 - f1_m: 0.9915 - precision_m: 0.986 - ETA: 26s - loss: 0.0482 - accuracy: 0.9920 - f1_m: 0.9915 - precision_m: 0.9862 - recall_m: 0.997 - ETA: 26s - loss: 0.0481 - accuracy: 0.9920 - f1_m:  - ETA: 23s - loss: 0.0468 - accuracy: 0.9921 - f1_m: 0.9917 - precision_m: 0.986 - ETA: 22s - loss: 0.0462 - accuracy: 0.9921 - f1_m: 0.9916 - precision_m: 0.9863 - re - ETA: 22s - loss: 0.0458 - accuracy: 0.9921 - f1_m: 0.9916 - precision_m: 0.9863 - recall_ - ETA: 21s - loss: 0.0455 - accuracy: 0.9921 - f1_m: 0.9916 - precision_m: 0.9863 - recall_ - ETA: 21s - loss: 0.0453 - accuracy: 0.9921 - f1_m: 0.9917 - precision_m: 0.9863 - recall_m: 0. - ETA: 20s - loss: 0.0451 - accuracy: 0.9921 - f1_m: 0.9916 - precision_m: 0.9863  - ETA: 19s - loss: 0.0445 - accuracy: 0.9921 - f1_m: 0.9917 - precision_m: 0.9864 - recal - ETA: 19s - loss: 0.0551 - accuracy: 0.9921 - f - ETA: 16s - loss: 0.0543 - accuracy: 0.9917 - f1_m: 0.9913 - precision_m: 0.9856 - recall_m: - ETA: 16s - loss: 0.0541 - accuracy: 0.9917 - f1_m: 0.9912 - precision_m: 0.9856 - recall_m: 0. - ETA: 16s - loss: 0.0540 - accuracy: 0.9916 - f1_m: 0.9912 - precision_m: 0.9855 - recal - ETA: 15s - loss: 0.0536 - accuracy: 0.9916 - f1_m: 0.9912 - precision_m: 0.9854 - recal - ETA: 14s - loss: 0.0532 - accuracy: 0.9916 - f1_m: 0.9912 - precision_m - ETA: 13s - loss: 0.0524 - accuracy: 0.9917 - f1_m: 0.9912 - precision_m: 0.9855 - recall_ - ETA: 13s - loss: 0.0525 - accuracy: 0.9916 - f1_m: 0.9912 - precision_m: 0.9854 - recall_m - ETA: 12s - loss: 0.0523 - accurac - ETA: 9s - loss: 0.0506 - accuracy: 0.9917 - f1_m: 0.9 - ETA: 8s - loss: 0.0499 - accuracy: 0.9917 - f1_m: 0.9913 - precision_m: 0.985 - ETA: 7s - loss: 0.0496 - accuracy: 0.9917 - f1_m: 0.9913 - precisio - ETA: 7s - loss: 0.0491 - accuracy: 0.9918 - f1_m: 0.9913 - precision_m: - ETA: 6s - loss: 0.0489 - accuracy: 0.9918 - f1_m: 0.9914 - precision_m: 0 - ETA: 5s - loss: 0.0487 - accuracy: 0.9918 - - ETA: 4s - loss: - ETA: 2s - loss: 0.0489 - accuracy: 0.9915 - f1_m: 0.9911 - precision_m: 0.9852 - recall_m: 0.99 - ETA: 2s - loss: 0.0489 - accuracy: 0.9915 - f1_m: 0.991 - ETA: 1s - loss: 0.0485 - accuracy: 0.9915 - f1_m: 0.9911 - precision_m: 0.9851 - reca - ETA: 0s - loss: 0.0483 - accuracy: 0.9915 - f1_m: 0.9911 - precision_m: 0.9852 - recall_m: 0.99 - ETA: 0s - loss: 0.0483 - accuracy: 0.9915 - f1_m: 0.9911 - precision_m: 0.9851 - recall - ETA: 0s - loss: 0.0482 - accuracy: 0.9915 - f1_m: 0.9911 - precision_m: 0.9852 - reca - ETA: 0s - loss: 0.0482 - accuracy: 0.9915 - f1_m: 0.9911 - precision_m: 0.9852 - re\n",
      "Epoch 6/10\n",
      "21324/21324 [==============================] - 74s 3ms/step - loss: 0.0422 - accuracy: 0.9911 - f1_m: 0.9906 - precision_m: 0.9853 - recall_m: 0.9969 - val_loss: 0.0101 - val_accuracy: 0.9985 - val_f1_m: 0.9984 - val_precision_m: 0.9977 - val_recall_m: 0.9993 precisio - ETA: 1:00 - loss: 0.0284 - accuracy: 0.9913 - f1_m: 0.9910 - precisi - ETA: 59s - loss: 0.0294 - accuracy: 0.9913 - f1_m: 0.9910 - precision_m: 0.9844 - recall_m: 0.9 - ETA: 59s - loss: 0.0305 - accuracy: 0.9910 - f1_m: 0.9907 - precision_m: 0.9838 - recall - ETA: 58s - loss: 0.0291 - accuracy: 0.9912 - f1_m: 0.9909 - precision_m: 0.9841 - recall_m: 0 - ETA: 58s - loss: 0.0292 - accuracy: 0.9910 - f1_m: 0.9907 - precision_m: 0.9838 - recall_m: 0.998 - ETA: 58s - loss: 0.0291 - accuracy: 0.9910 - f1_m: 0.9907 - precision_m: 0.983 - ETA: 57s - loss: 0.0308 - accuracy: 0.9908 - f1_m: 0.9904 - precision_m: 0.9832 - recall_m: 0. - ETA: 56s - loss: 0.0306 - accuracy: 0.9908 - f1_m: 0.9904 - precis - ETA: 55s - loss: 0.0304 - accuracy: 0.9904 - f1_m: 0.9900 - precision_m: 0.9825 - recall_m:  - ETA: 54s - loss: 0.0298 - accuracy: 0.9905 - f1_m: 0.9900 - precision_m: 0.9825 - reca - ETA: 54s - loss: 0.0302 - accuracy: 0.9906 - f1_m: 0.9901 - precision_m: 0.9827 - recall_m: 0 - ETA: 54s - loss: 0.0303 - accuracy: 0.9906 - f1_m: 0.9901 - precision_m: 0.9828 - recall_m:  - ETA: 53s - loss: 0.0297 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9831 - recall - ETA: 53s - loss: 0.0293 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9831 - recall_m: - ETA: 53s - loss: 0.0289 - accuracy: 0.9908 - f1_m: 0.9904 - precision_m: 0. - ETA: 52s - loss: 0.0288 - accuracy: 0.9909 - f1_m: 0.9903 - precision_m: 0.9831 - recall_m: 0.998 - ETA: 52s - loss: 0.0288 - accuracy: 0.9909 - f1_m: 0.9903 - precision_m: 0.9 - ETA: 50s - loss: 0.0304 - accuracy: 0.9905 - f1_m: 0.9900 - precision_m: 0.9829 - recall_m: 0.998 - ETA: 50s - loss: 0.0304 - accuracy: 0.9905 - f1_m: 0.9899 - precision_m: 0.9829  - ETA: 49s - loss: 0.0317 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.9832 - recall_ - ETA: 49s - loss: 0.0315 - accuracy: 0.9905 - f1_m: 0.9899 - precision_m - ETA: 48s - loss: 0.0323 - accuracy: 0.9903 - f1_m: 0.9896 - precision_m: 0.9837 - reca - ETA: 47s - loss: 0.0317 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0 - ETA: 46s - loss: 0.0304 - accuracy: 0.99 - ETA: 43s - loss: 0.0458 - accuracy: 0.9901 - f1_m: 0.9894 - precision_m: 0.9845 - recall_m:  - ETA: 43s - loss: 0.0454 - accuracy: 0.9901 - f1_m: 0.9895 - p - ETA: 41s - loss: 0.0445 - accuracy: 0.9902 - f1_m: 0.9896  - ETA: 40s - loss: 0.0439 - accuracy: 0.9902 - f1_m: 0.9896 - - ETA: 38s - loss: 0.0423 - acc - ETA: 34s - loss: 0.0405 - accuracy: 0.9907 - f1_m: 0.9901 - precision_m: 0.9848 - recall_ - ETA: 34s - loss: 0.0402 - accuracy: 0.9907 - f1_m: 0.9902 - precision_m: 0.9849 - recall_m: 0.9 - ETA: 34s - loss: 0.0401 - accuracy: 0.9907 - f1_m: 0.9902 - precision_m: 0.9849 - recall_ - ETA: 33s - loss: 0.0407 - accuracy: 0.9908 - f1_m: 0.9902 - precision_m: 0.9849 - r - ETA: 32s - loss: 0.0402 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9850 - r - ETA: 32s - loss: 0.0397 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9850 - recall_m: 0.99 - ETA: 31s - loss: 0.0397 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9850 - recall_m: 0.996 - ETA: 31s - loss: 0.0397 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9850 - recall_m:  - ETA: 31s - loss: 0.0395 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9851 - recall_m: 0 - ETA: 31s - loss: 0.0394 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9850 - recall_m: 0.9 - ETA: 31s - loss: 0.0394 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9850 - re - ETA: 30s - loss: 0.0389 - accuracy: 0.9910 - f1_m: 0.9905 - precision_m: 0. - ETA: 29s - loss: 0.0385 - accuracy: 0.9910 - f1_m: 0.9904 - precision_m: 0.9850 - recall_m - ETA: 28s - loss: 0.0384 - accuracy: 0.9910 - f1_m: 0.9904 - precision_m: 0.9850 - ETA: 27s - loss: 0.0504 - accuracy: 0.9905 - f1_m: 0.9898 - precision_m: 0.9849 - reca - ETA: 27s - loss: 0.0508 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.9848 - recall_m: 0. - ETA: 26s - loss: 0.0508 - accuracy: 0.9904 - f1_ - ETA: 24s - loss: 0.0502 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.98 - ETA: 22s - loss: 0.0506 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.9846 - recall_m: 0. - ETA: 22s - loss: 0.0506 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.9846 - recall_m: 0.996 - ETA: 22s - loss: 0.0506 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.9846 - recall_m: 0. - ETA: 22s - loss: 0.0505 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.9846 - recall_ - ETA: 22s - loss: 0.0502 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.9847 - recall_m: 0 - ETA: 21s - loss: 0.0501 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.9847 - recal - ETA: 21s - loss: 0.0498 - accuracy: 0.9905 - f1_m: 0.9899 - precision_m: 0.9847 - recall_m:  - ETA: 20s - loss: 0.0497 - accuracy: 0.9905 - f1_m: 0.9899 - precision_m: 0.9847 - recall_m: 0.9 - ETA: 20s - loss: 0.0496 - accuracy: 0.9905 - f1 - ETA: 18s - loss: 0.0483 - accuracy: 0.9905 - f1_m: 0.9900 - precision_m: 0.9847 - recall_m: 0.996 - ETA: 17s - loss: 0.0482 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.984 - ETA: 16s - loss: 0.0478 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.9847 - r - ETA: 16s - loss: 0.0475 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.9 - ETA: 14s - loss: 0.0473 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.9846  - ETA: 14s - loss: 0.0469 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.9846 - recal - ETA: 13s - loss: 0.0465 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.9847 - r - ETA: 12s - loss: 0.0467 - accuracy: 0.9907 - f1_m: 0.9901 - precisio - ETA: 11s - loss: 0.0459 - accuracy: 0.9907 - f1_m: 0.9902 - precision_m: 0.9848 - recall_m: 0 - ETA: 10s - loss: 0.0458 - accuracy: 0.9907 - f1_m: 0.9902 - precision_m: 0.9848 - recall_m: 0. - ETA: 10s - loss: 0.0457 - accuracy: 0.9907 - f1_m: 0.9902 - precision_m: 0.9848 - recall - ETA: 10s - loss: 0.0458 - accuracy: 0.9908 - f1_m: 0. - ETA: 8s - loss: 0.0451 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9849 -  - ETA: 8s - loss: 0.0451 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9849 -  - ETA: 8s - loss: 0.0450 - accura - ETA: 6s - loss: 0.0444 - accuracy: 0.9909 - f1_m: 0.9903 - precision_m: 0.9849 -  - ETA: 5s - loss: 0.0442 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.985 - ETA: 5s - loss: 0.0439 - accuracy: 0.9910 - f - ETA: 4s - loss: 0.0432 - accuracy: 0.9910 - f1_m: 0.9905 - precision_m: 0.9851 - recall_m:  - ETA: 3s - loss: 0.0431 - accuracy: 0.9910 - f1_m: 0.9905 - precision_m: 0.985 - ETA: 3s - loss: 0.0431 - accuracy: 0.9911 - f1_m: 0.9905 - precision_ - ETA: 2s - loss: 0.0428 - accuracy: 0.9911 - f1_m: 0.9906 - precision_m: 0 - ETA: 1s - loss: 0.0426 - accuracy: 0.9911 - f1_m: 0.9906 - precision_m: 0.9852 - recall_m:  - ETA: 1s - loss: 0.0425 - accuracy: 0.9911 - f1_m: 0.9906 - precision_m: 0.9852 - recall_m: 0.99 - ETA: 1s - loss: 0.0425 - accuracy: 0.9911 - f1_m: 0.9906 - precision_m: 0.9852 - recall_m: 0. - ETA: 1s - loss: 0.0424 - accuracy: 0.9911 - f1_m: 0.9906 - precisio - ETA: 0s - loss: 0.0424 - accuracy: 0.9911 - f1_m: 0.9906 - precision_m: 0.9852 - recall_m: 0.99 - ETA: 0s - loss: 0.0424 - accuracy: 0.9911 - f1_m: 0.9906 - precision_m: 0.9852  - ETA: 0s - loss: 0.0423 - accuracy: 0.9911 - f1_m: 0.9906 - precision_m: 0.9853 - recall - ETA: 0s - loss: 0.0422 - accuracy: 0.9911 - f1_m: 0.9906 - precision_m: 0.9853 - recall_m: 0.\n",
      "Epoch 7/10\n",
      "21324/21324 [==============================] - 75s 4ms/step - loss: 0.0386 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.9846 - recall_m: 0.9963 - val_loss: 0.0201 - val_accuracy: 0.9974 - val_f1_m: 0.9972 - val_precision_m: 0.9951 - val_recall_m: 0.9997loss: 0.0169 - accuracy: 0.9900 - f1_m: 0.9892 - precision_m: 1.0000 - recall_m: 0. - ETA: 1:05 - loss: 0.0159 - accuracy: 0.9919 - f1_m: 0.9912 - precision_m: 1.0000  - ETA: 1:04 - loss: 0.0502 - accuracy:  - ETA: 1:01 - loss: 0.0378 - accuracy: 0.9933 - f1_m: 0.9930 - precision_m: 0.9971 - recall - ETA: 1:01 - loss: 0.0357 - accuracy: 0.9932 - f1_m: 0.9929 - precision_m: 0.9973 - recall_m:  - ETA: 1:01 - loss: 0.0344 - accuracy: 0.9932 - f1_m: 0.9929 - ETA: 59s - loss: 0.0369 - accuracy: 0.9935 - f1_m: 0.9932 - precision_m: 0.9970 - recall_m: 0.990 - ETA: 59s - loss: 0.0368 - accuracy: 0.9935 - f1_m: 0.9932 - precision_m: 0.9970 - recall_m:  - ETA: 59s - loss: 0.0355 - accuracy: 0.9934 - f1_m: 0 - ETA: 59s - loss: 0.0338 - accuracy: 0.9928 - f1_m: 0.9925 - precision_m: 0.9930 - reca - ETA: 59s - loss: 0.0326 - accuracy: 0.9928 - f1_m: 0.9925 - precision_m: 0.9925 - rec - ETA: 59s - loss: 0.0324 - accuracy: 0.9926 - f1_m: 0.9922 - precision_m: 0.9916 - recall_m: 0.993 - ETA: 59s - loss: 0.0322 - accuracy: 0.9926 - f1_m: 0.9923 - precision_m: 0.9916 - recall_m - ETA: 59s - loss: 0.0316 - accuracy: 0.9926 - f1_m: 0.9923 - precision_m: 0.9913 - reca - ETA: 58s - loss: - ETA: 54s - loss: 0.0331 - accuracy: 0.9929 - f1_m: 0.9926 - precision_m: 0.9915 - recall_m - ETA: 54s - loss: 0.0326 - accuracy: 0.9929 - f1_m: 0.9926 - precision_m: 0.9914 - rec - ETA: 53s - loss: 0.0318 - accuracy: 0. - ETA: 49s - loss: 0.0339 - accuracy: 0.9919 - f1_m: 0.9915 - precision_m - ETA: 47s - loss: 0.0372 - accuracy: 0.9916 - f1_m: 0.9911 - precision_m: 0.9877 - recall_m: 0.995 - ETA: 47s - loss: 0.0372 - accuracy: 0.9916 - f1_m: 0.9911 - precision_m: 0.9877 - r - ETA: 47s - loss: 0.0370 - accuracy: 0.9916 - f1_m: 0.9911 - precision_m: 0. - ETA: 46s - loss: 0.0372 - accuracy: 0.9915 - f1_m: 0.9910 - precision_m: 0.9874 -  - ETA: 45s - loss: 0.0369 - accuracy: 0.9914 - f1_m: 0.9909 - precision_m: 0. - ETA: 43s - loss: 0.0364 - accuracy: 0.9915 - f1_m: 0.9910 - precision_m: 0.9870 - recall_m: - ETA: 43s - loss: 0.0362 - accuracy: 0.9915 - f1_m: 0.9910 - precision_m: 0.9 - ETA: 42s - loss: 0.0355 - accuracy: 0.9914 - f1_m: 0.9909 - precision_m: 0. - ETA: 40s - loss: 0.0357 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9864 - recall_m:  - ETA: 40s - loss: 0.0356 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0 - ETA: 38s - loss: 0.0354 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9863 - recall_m - ETA: 38s - loss: 0.0351 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9863 - recall_m: 0.996 - ETA: 38s - loss: 0.0351 - accuracy: 0.9913 - f1_m: 0.9908 - precision_ - ETA: 37s - loss: 0.0347 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0 - ETA: 35s - loss: 0.0344 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9861 - recall_m: 0.99 - ETA: 35s - loss: 0.0343 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9861 - recall_m: 0.99 - ETA: 35s - loss: 0.0343 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9861 - recall_m: 0.99 - ETA: 35s - loss: 0.0343 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9860 - recall_m:  - ETA: 35s - loss: 0.0341 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m:  - ETA: 33s - loss: 0.0336 - accuracy: 0.9913 - f1_m: 0.9909 - precision_ - ETA: 32s - loss: 0.0334 - accuracy: 0.9912 - f1_m: 0.9908 - precision_m: 0.9858 - recall_m: - ETA: 31s - loss: 0.0334 - accuracy: 0.9912 - f1_m: 0.9907 - precision_m: 0 - ETA: 30s - loss: 0.0333 - accuracy: 0.9912 - f1_m: 0.9907 - precision_m: 0.9856 - recall_m: - ETA: 30s - loss: 0.0333 - accuracy: 0.9912 - f1_m: 0.9907 - precision_m: 0.9856 - recall_m: 0.9 - ETA: 29s - loss: 0.0333 - accuracy: 0.9912 - f1_m: 0.9907 - precision_m: 0.9856 - recall_m:  - ETA: 29s - loss: 0.0333 - accuracy: 0.9912 - f1_m: 0.9907 - precision_m: 0.9855 - recall_m: 0.99 - ETA: 29s - loss: 0.0334 - accuracy: 0.9912 - f1_m: 0.9907 - precision_m: 0.9855 - recall_m: - ETA: 29s - loss: 0.0334 - accuracy: 0.9912 - f1_m: 0.9907 - precision_m: 0.9855 - reca - ETA: 28s - loss: 0.0345 - accuracy: 0.9910 - f1_m: 0.9905 - precision_m: 0.9 - ETA: 27s - loss: 0.0353 - accuracy: 0.9907 - f1_m: 0.9902 - precision_m: 0.9849 - recall - ETA: 26s - loss: 0.0353 - accuracy: 0.9907 - f1_m: 0.9902 - precision_m: 0.9851  - ETA: 25s - loss: 0.0353 - accuracy: 0.9907 - f1_m: 0.9902 - precision_m: 0.9853 - recall_m - ETA: 25s - loss: 0.0352 - accuracy: 0.9908 - f1_m: 0.9902 - precision_m: 0.9854 - recall_m - ETA: 24s - loss: 0.0351 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9856 - recall_m: 0.99 - ETA: 24s - loss: 0.0351 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9856 - recall_ - ETA: 24s - loss: 0.0352 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9857 - recall_m: 0. - ETA: 23s - loss: 0.0395 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9858 - recall_m: 0 - ETA: 23s - loss: 0.0398 - accuracy: 0.9906 - f1_m: 0.9901 - precision_m: 0.9856 - recall_m: - ETA: 23s - loss: 0.0398 - accuracy: 0.9906 - f1_m: 0.9900 - precisio - ETA: 21s - loss: 0.0396 - accuracy: 0.9903 - f1_m: 0.9897 - precisi - ETA: 19s - loss: 0.0390 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9847 - recall_m - ETA: 19s - loss: 0.0389 - accuracy: 0.9903 - f1_m: 0.9898 - precision_m: 0.9848 - rec - ETA: 18s - loss: 0.0387 - accuracy: 0.9903 - f1_m: 0.9898 - precision_m: 0.9847 - - ETA: 18s - loss: 0.0388 - accuracy: 0.9903 - f1_m: 0.9898 - precision_m: 0.9847 - recall_m - ETA: 17s - loss: 0.0386 - accuracy: 0.9903 - f1_m: 0.9898 - precision - ETA: 16s - loss: 0.0384 - accuracy: 0.9904 - f1_m: 0.9899 - precision_m: 0.9848 - recall_m: 0. - ETA: 15s - loss: 0.0384 - accuracy: 0.9904 - f1_m: 0.9899 - precision_m: 0.9848 - recall_m: 0.99 - ETA: 15s - loss: 0.0383 - accuracy: 0.9904 - f1_m: 0.9899 - - ETA: 13s - loss: 0.0407 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.9847 - reca - ETA: 12s - loss: 0.0406 - accuracy: 0.9903 - f1_m: 0.9898 - precision_m: 0.98 - ETA: 11s - loss: 0.0405 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9844 - recal - ETA: 11s - loss: 0.0404 - accuracy: 0.9902 - f1_m: 0.9897 - precision_m: 0.9843 - recall_m:  - ETA: 10s - loss: 0.0404 - accuracy: 0.9902 - f1_m: 0.9897 -  - ETA: 9s - loss: 0.0399 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9843 -  - ETA: 9s - loss: 0.0398 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9843 - reca - ETA: 8s - loss: 0.0398 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9843 - recall_m: 0.99 - ETA: 8s - loss: 0.0398 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: - ETA: 5s - loss: 0.0389 - accuracy: 0.99 - ETA: 3s - loss: 0.0388 - accuracy: 0.9903 - f - ETA: 2s - loss: 0.0389 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9841 - recall_m:  - ETA: 2s - loss: 0.0389 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9841 - re - ETA: 1s - loss: 0.0391 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9842 - recall_m:  - ETA: 1s - loss: 0.0390 - accuracy: 0.9903 - f1_m: 0.9897 -  - ETA: 0s - loss: 0.0388 - accuracy: 0.9903 - f1_m: 0.9898 - precision_m: 0 - ETA: 0s - loss: 0.0386 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.9846 - recall_m: 0.99\n",
      "Epoch 8/10\n",
      "21324/21324 [==============================] - 75s 4ms/step - loss: 0.0391 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9858 - recall_m: 0.9962 - val_loss: 0.0112 - val_accuracy: 0.9975 - val_f1_m: 0.9974 - val_precision_m: 0.9962 - val_recall_m: 0.9989acy: 0.9911 - f1_m: 0.9908 - precision_m: 0.9877 - recall_m: - ETA: 55s - loss: 0.0345 - accuracy: 0.9911 - f1_m: 0.9908 - precision_m: 0.9876 - recall_m: 0.99 - ETA: 54s - loss: 0.0344 - accuracy: 0.9911 - f1_m: 0.9908 - precision_m: 0.9 - ETA: 53s - loss: 0.0341 - accuracy: 0.9911 - f1_m: 0.9908 - precision_m: 0.9873 - recall_m: 0.9 - ETA: 53s - loss: 0.0338 - accuracy: 0.9912 - f1_m: 0.9909 - precision_m: 0.9875 - recall_m: - ETA: 52s - loss: 0.0363 - accuracy: 0.9912 - f1_m: 0.9908 - precision_m: 0.9878 -  - ETA: 51s - loss: 0.0359 - accuracy: 0.9912 - f1_m: 0.9908 - pr - ETA: 49s - loss: 0.0344 - accuracy: 0.9913 - f1_m: 0.9909 - precision_m: 0.9879 - recall_m: 0 - ETA: 49s - loss: 0.0342 - accuracy: 0.9913 - f1_m: 0.9909 - precision_m: 0.9878 - recall - ETA: 49s - loss: 0.0335 - accuracy: 0.9913 - f1_m: 0.9910 - precision_m: 0.987 - ETA: 47s - loss: 0.0325 - accuracy: 0.9914 - f1_m: 0.9911 - precision_m: 0. - ETA: 46s - loss: 0.0325 - accuracy: 0.9914 - f1_m: 0.9910 - - ETA: 44s - loss: 0.0331 - accura - ETA: 41s - loss: 0.0339 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9872 - recall_m: 0.994 - ETA: 41s - loss: 0.0339 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9872 - rec - ETA: 40s - loss: 0.0337 - accuracy: 0.9910 - f1_m: 0.9904 - precision_m: 0.9875 - recall_m - ETA: 39s - loss: 0.0334 - accuracy: 0.9910 - f1_m: 0.9905 - precision_m: 0.9877 - recal - ETA: 39s - loss: 0.0351 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9879 - recall_m: 0.99 - ETA: 39s - loss: 0.0351 - accuracy: 0.9909 - f1_m: 0.9903 - precision_m: 0.9880  - ETA: 38s - loss: 0.0387 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.9883 - recal - ETA: 37s - loss: 0.0386 - accuracy: 0.9905 - f1_m: 0.9899 - precision_m: 0.9881 -  - ETA: 36s - loss: 0.0446 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9878 - recall_m: 0 - ETA: 36s - loss: 0.0444 - accuracy: 0.9903 - f1_m: 0.9898 - precision_m: 0.9877 - recall_m: 0.99 - ETA: 36s - loss: 0.0444 - accuracy: 0.9903 - f1_m: 0.9898 - precision_m: 0.9877 - recall_m: 0.993 - ETA: 36s - loss: 0.0444 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9877 - recall_m: 0.993 - ETA: 36s - loss: 0.0444 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9877 - recall_m: 0.99 - ETA: 35s - loss: 0.0443 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9877 - recall_m - ETA: 35s - loss: 0.0455 - accuracy: 0.9902 - f1_m: 0.9896 - precision_ - ETA: 34s - loss: 0.0459 - accuracy: 0.9901 - f1_m: 0.9895 - precision_m: 0.9869 - recall_m:  - ETA: 33s - loss: 0.0460 - accuracy: 0.9901 - f1_m: 0.9895 - precision_m: 0.9868 - recall_m: 0.993 - ETA: 33s - loss: 0.0460 - accuracy: 0.9900 - f1_m: 0.9894 - precision_m: 0.9867 - recall_m: 0.99 - ETA: 33s - loss: 0.0459 - accuracy: 0.9900 - f1_m: 0.9894 - precision_m: 0.9867 - reca - ETA: 32s - loss: 0.0456 - accuracy: 0.9901 - f1_m: 0.9895 - precision_m: 0.9867 - recall_m: 0. - ETA: 32s - loss: 0.0455 - accuracy: 0.9901 - f1_m: 0.9895 - precision_m: 0.9867 - recall_m: 0.9 - ETA: 32s - loss: 0.0455 - accuracy: 0.9901 - f1_m: 0.9895 - precision_m: 0.9867 - recal - ETA: 31s - loss: 0.0470 - accuracy: 0.9901 - f1_m: 0.9895 - precision_m - ETA: 30s - loss: 0.0473 - accuracy: 0.9901 - f1_m: 0.9895 - precision_m: 0.9864 - recall_m: - ETA: 30s - loss: 0.0472 - accuracy: 0.9901 - f1_m: 0.9894 - prec - ETA: 28s - loss: 0.0463 - accuracy: 0.9902 - f1_m: 0.9895 - precision_m: 0.9862 - recall_m: 0.994 - ETA: 28s - loss: 0.0463 - accuracy: 0.9902 - f1_m: 0.9895 - precision_m: 0.9862 - recall - ETA: 27s - loss: 0.0460 - accuracy: 0.9902 - f1_m: 0.9896 - precision_m: 0.9862 - recall_m: 0 - ETA: 27s - loss: 0.0458 - accuracy: 0.9902 - f1_m: 0.9896 - precision_m: 0.9863  - ETA: 26s - loss: 0.0455 - accuracy: 0.9902 - f1_m: 0.9896 - precision_m: 0.9862 - recall_m: 0.99 - ETA: 26s - loss: 0.0455 - accuracy: 0.9902 - f1_m: 0.9896 - precision_m: 0.9861 - recall_m: 0 - ETA: 26s - loss: 0.0461 - accuracy: 0.9902 - f1_m: 0.9895 - precisio - ETA: 24s - loss: 0.0460 - accuracy: 0.9901 - f1_m: 0.9895 - precision_m: 0.9858 - rec - ETA: 23s - loss: 0.0458 - accuracy: 0.9901 - f1_m: 0.9895 - precision_m: 0.9857 -  - ETA: 22s - loss: 0.0454 - accuracy: 0.9901 - f1_m: 0.9895 - precision_m: 0 - ETA: 21s - loss: 0.0447 - accuracy: 0.9902 - f1_m: 0.9896 - precision - ETA: 20s - loss: 0.0443 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9857 - re - ETA: 19s - loss: 0.0443 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9857 - recall_m: 0. - ETA: 19s - loss: 0.0442 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9857 - recall_m: 0.9 - ETA: 19s - loss: 0.0442 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9857 - recall_m: 0 - ETA: 18s - loss: 0.0440 - accuracy: 0.9903 - f1_m: 0.9897 - precision_m: 0.9857 - recall_m: - ETA: 18s - loss: 0.0440 - accuracy: 0.9903  - ETA: 15s - loss: 0.0427 - accuracy: 0.9905 - f1_m: 0.9899 - p - ETA: 13s - loss: 0.0430 - accuracy: 0.9905 - f1_m: 0.9900 - precision_m: 0.9857 - recall - ETA: 13s - loss: 0.0427 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.9858 - recall_m: 0.995 - ETA: 13s - loss: 0.0427 - accuracy: 0.9906 - f1_m: 0.9900 - prec - ETA: 11s - loss: 0.0421 - accuracy: 0.9906 - f1_m: 0.9901 - precision_m: 0.9858 - reca - ETA: 10s - loss: 0.0418 - accuracy: 0.9907 - f1_m: 0.9901 - precision_m: 0.9858 - recall_m: - ETA: 10s - loss: 0.0416 - accuracy: 0.9907 - f1_m: 0.9902 - precision_m: 0.9859 - recall_m: 0.995 - ETA: 10s - loss: 0.0416 - accuracy: 0.9907 - f1_m: 0.9902 - precision_m: 0.9859 - reca - ETA: 9s - loss: 0.0415 - accuracy: 0.9907 - f1_m: 0.9902 - precision_m: 0.9859 - recall - ETA: 9s - loss: 0.0414 - accuracy: 0.9907 - f1_m: 0 - ETA: 8s - loss: 0.0413 - accuracy: 0.9907 - f1_m: 0.9902 - precision_m: 0.9858 - re - ETA:  - ETA: 5s - loss: 0.0406 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9858 - reca - ETA: 5s - loss: 0.0405 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9858 - recall_m: 0. - ETA: 4s - loss: 0.0404 - accuracy: 0.9908 - f1_m: 0 - ETA: 3s - loss: 0.0401 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9859 - recall_m:  - ETA: 3s - loss: 0.0401 - accuracy: 0.9909 - f1_m: 0.990 - ETA: 2s - l - ETA: 0s - loss: 0.0392 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9858 - recall_m:  - ETA: 0s - loss: 0.0391 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9858 - recall_m: 0.\n",
      "Epoch 9/10\n",
      "21324/21324 [==============================] - 74s 3ms/step - loss: 0.0453 - accuracy: 0.9913 - f1_m: 0.9909 - precision_m: 0.9890 - recall_m: 0.9939 - val_loss: 0.0220 - val_accuracy: 0.9940 - val_f1_m: 0.9937 - val_precision_m: 0.9890 - val_recall_m: 0.999059s - loss: 0.0457 - accuracy: 0.9895 - f1_m: 0.9889 - precision_m:  - ETA: 57s - loss: 0.0410 - accuracy: 0.9903 - f1_m: 0.9898 - precision_m: 0. - ETA: 56s - loss: 0.0375 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9836 - recall_m: 0.998 - ETA: 56s - loss: 0.0373 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9836 - recall_m: 0.99 - ETA: 56s - loss: 0.0370 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9837 - recall_m - ETA: 55s - loss: 0.0365 - accuracy: 0.9909 - f1_m: 0.9905 - precision_m: 0.9838 - recall_ - ETA: 55s - loss: 0.0353 - accuracy: 0.9910 - f1_m: 0.9906 - precision_m: 0.9840 - recall_m: 0.998 - ETA: 55s - loss: 0.0352 - accuracy: 0.9910 - f1_m: 0.9906 - precision_m: 0.9840 - recall_m: 0. - ETA: 54s - loss: 0.0368 - accuracy: 0.9910 - f1_m: 0.9905 - precision_m: 0.9839 - recall_m: 0.99 - ETA: 54s - loss: 0.0365 - accuracy: 0.9910 - f1_m: 0.9906 - precision_m: 0.9840 - recall_m: 0.998 - ETA: 54s - loss: 0.0363 - accuracy: 0.9910 - f1_m: 0.9906 - precision_m: 0.9840 - recall_m: 0.9 - ETA: 54s - loss: 0.0359 - accuracy: 0.9911 - f1_m: 0.9907 - precision_m: 0.9841 - recall_m: 0.998 - ETA: 54s - loss: 0.0360 - accuracy: 0.9911 - f1_m: 0.9 - ETA: 52s - loss: 0.0336 - accuracy: 0.9914 - f1_m: 0.9910 - precision_m: 0.9846 - recall_m: 0.9 - ETA: 52s - loss: 0.0335 - accuracy: 0.9915 - f1_m: 0.9910 - precision_m: 0.9846 - re - ETA: 51s - loss: 0.0338 - accuracy: 0.9916 - f1_m: 0.9911 - precision - ETA: 50s - loss: 0.0326 - accuracy: 0.9917 - f1_m: 0.9913 - precision_m: 0.9849 - recall_m: 0. - ETA: 50s - loss: 0.0323 - accuracy: 0.9918 - f1_m: 0.9913 - precision_m: 0.9850 - recall_m: 0.998 - ETA: 49s - loss: 0.0322 - accuracy: 0.9918 - f1_m: 0.9914 - precision - ETA: 48s - loss: 0.0319 - accuracy: 0.9918 - f1_m: 0.9914 - precision_m: 0.9850 - recall_m: 0 - ETA: 48s - loss: 0.0318 - accuracy: 0.9918 - f1_m: 0.9914 - precision_m: 0. - ETA: 47s - loss: 0.0321 - accuracy: 0.9917 - f1_m: 0.9912 - precision_m: 0.9848 - recall_m - ETA: 46s - loss: 0.0323 - accuracy: 0.9917 - f1_m: 0.9912 - precision_m: 0.9848 - recall_m:  - ETA: 46s - loss: 0.0323 - accuracy: 0.9916 - f1_m: 0.9911 - precision_m: 0.9847 - recall_ - ETA: 46s - loss: 0.0321 - accuracy: 0.9916 - f1_m: 0.9911 - precision_m: 0.9847 - recall_m: 0.998 - ETA: 45s - loss: 0.0320 - accuracy: 0.9916 - f1_m: 0.9912 - precision_m: 0.9848 -  - ETA: 45s - loss: 0.0317 - accuracy: 0.9917 - f1_m: 0.9912 - precision_m: 0.9849 - recall_ - ETA: 44s - loss: 0.0315 - accuracy: 0.9917 - f1_m: 0.9913 - precision_m: 0.9849 - recall_m: 0. - ETA: 44s - loss: 0.0315 - accuracy: 0.9917 - f1_m: 0.9913 - precision_m: 0.9849 - recall - ETA: 44s - loss: 0.0313 - accuracy: 0.9918 - f1_m: 0.9913 - precision_m: 0.9850 - recall_m: 0 - ETA: 43s - loss: 0.0311 - accuracy: 0.9918 - f1_m: 0.9913 - precision_m: 0.98 - ETA: 42s - loss: 0.0315 - accuracy: 0.9917 - f1_m: 0.9912 - precision_m: 0.9848 - recall_m: 0.998 - ETA: 42s - loss: 0.0315 - accuracy: 0.9917 - f1_m: 0.9912 - precision_m: 0.9 - ETA: 41s - loss: 0.0310 - accuracy: 0.9918 - f1_m: 0.9913 - precision_m: 0.9850  - ETA: 40s - loss: 0.0339 - accuracy: 0.9917 - f1_m: 0.9912 - precision_m: 0.9848 - recall_m: - ETA: 40s - loss: 0.0338 - accuracy: 0.9917 - f1_m: 0.9912 - precision_m: 0.9849 -  - ETA: 39s - loss: 0.0355 - accuracy: 0.9918 - f1_m: 0.9913 - precision_m: 0.9 - ETA: 38s - loss: 0.0353 - accuracy: 0.9918 - f1_m: 0.9914 - precision_m: 0.9858  - ETA: 37s - loss: 0.0347 - accuracy: 0.9920 - f1_m: 0.9915 - precision_m: 0.9862 - - ETA: 36s - loss: 0.0343 - accuracy: 0.9921 - f1_m: 0.9916 - precision_m: 0.9866 - recall_m: 0. - ETA: 36s - loss: 0.0341 - accuracy: 0.9921 - f1_m: 0.9916 - precision_m: 0.9867 - recall_m: 0. - ETA: 35s - loss: 0.0342 - accuracy: 0.9921 - f1_m: 0.9916 - precision_m: 0.9867 - re - ETA: 35s - loss: 0.0361 - accuracy: 0.9920 - f1_m: 0.9915 - precision_m: 0.9867 - recall_m: 0. - ETA: 34s - loss: 0.0361 - accuracy: 0.9919 - f1_m: 0.9914 - precision - ETA: 33s - loss: 0.0378 - accuracy: 0.9918 - f1_m: 0.9913 - prec - ETA: 31s - loss: 0.0402 - accuracy: 0.9914 - f1_m: 0.9909 - precision_m: 0.9862 - recall_m: 0.99 - ETA: 31s - loss: 0.0402 - accuracy: 0.9914 - f1_m: 0.9909 - precision_m: 0.9862 - recall_m: 0 - ETA: 31s - loss: 0.0402 - accuracy: 0.9914 - f1_m: 0.9909 - precision_m: 0.9863 - - ETA: 30s - loss: 0.0400 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9866 - recall_m: 0 - ETA: 30s - loss: 0.0398 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9867 - recall_m: 0. - ETA: 30s - loss: 0.0398 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9867 - recal - ETA: 29s - loss: 0.0396 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9869 - recall_m: 0. - ETA: 29s - loss: 0.0395 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9870 - reca - ETA: 28s - loss: 0.0393 - accuracy: 0.9913 - f1 - ETA: 26s - loss: 0.0410 - accuracy: 0.9915 - f1_m: 0.9909 - precision_m: 0.9879 - recall - ETA: 25s - loss: 0.0417 - accuracy: 0.9914 - f1_m: 0.9909 - precision_m:  - ETA: 24s - loss: 0.0416 - accuracy: 0.9914 - f1_m: 0.9909 - precision_m: 0.9882 - recall_m: 0.9 - ETA: 24s - loss: 0.0415 - accuracy: 0.9914 - f1_m: 0.9909 - precision_m: 0.9882 - reca - ETA: 23s - loss: 0.0413 - accuracy: 0.9914 - f1_m: 0.9909 - precision_m: 0.9883 - recal - ETA: 22s - loss: 0.0411 - accuracy: 0.9915 - f1_m: 0.9910 - precision_m: 0.9885 - recall_m:  - ETA: 22s - loss: 0.0411 - accuracy: 0.9915 - f1_m: 0.9910 - precision_m: 0.9885 - r - ETA: 21s - loss: 0.0409 - accuracy: 0.9916 - f1_m: 0.9911 - precision_m: 0 - ETA: 20s - loss: 0.0405 - accuracy: 0.9916 - f1_m: 0.9911 - precision_m: 0 - ETA: 19s - loss: 0.0403 - accuracy: 0.9916 - f1_m: 0.9911 - precision_m: 0.9891 - recal - ETA: 18s - loss: 0.0401 - accuracy: 0.9915 - f1_m: 0.9911 - precision_m: 0.9892 - recall_m - ETA: 18s - loss: 0.0400 - accuracy: 0.9916 - f1_m: 0.9911 - precision_m: 0.9893 - recall_m:  - ETA: 17s - loss: 0.0399 - accuracy: 0.9916 - f1_m: 0.9911 - precision_m: 0.9894 - re - ETA: 17s - loss: 0.0405 - accuracy: 0.9916 - f1_m: 0.9911 - precision_m: 0.9895 - recal - ETA: 16s - loss: 0.0404 - accuracy: 0.9916 - f1_m: 0.9911 - precision_m: 0.9895 - recall_m: 0.99 - ETA: 16s - loss: 0.0404 - accuracy: 0.9916 - f1_m: 0.9911 - precision_m: 0.9895 -  - ETA: 15s - loss: 0.0440 - accuracy: 0.9915 - f1_m: 0.9910 - precision_m: 0.9896 - reca - ETA: 15s - loss: 0.0437 - accuracy: 0.9915 - f1_m: 0.9910 - precision_m: 0. - ETA: 13s - loss: 0.0490 - accuracy: 0.9915 - f1_m: 0.9910 - precision_m: 0.9 - ETA: 12s - loss: 0.0487 - accuracy: 0.9914 - f1_m: 0.9910 - precision_ - ETA: 11s - loss: 0.0483 - accuracy: 0.9914 - f1_m:  - ETA: 9s - loss: 0.0479 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9889 - recall_m:  - ETA: 9s - loss: 0.0478 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9889  - ETA: 8s - loss: 0.0477 - accuracy: 0.9913 - f1_m: 0.9909 - precision_m: 0.989 - ETA: 8s - loss: 0.0474 - accuracy: 0.9913 - f1_m: 0.9909 - precision_m: 0.9891 - recall_m - ETA: 7s - loss: 0.0474 - accuracy: 0.9914 - f1_m: 0.9909 - precision_m: 0.9891 - recall_m:  - ETA: 7s - loss: 0.0473 - accuracy: 0.9914 - f1_m: 0.9909 - precision_m: 0.9892 - reca - ETA: 7s - loss: 0.0472 - accuracy: 0.9914 - f1_m: 0.9909 - pr - ETA: 6s - loss: 0.0469 - accuracy: 0.9914 - f1_m: 0.9909 - precision_m: 0.9894 - recall_m: 0.99 - ETA: 6s - l - ETA: 4s - loss: 0.0461 -  - ETA: 2s - loss: 0.0461 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9891 -  - ETA: 2s - loss: 0.0460 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9891 - reca - ETA: 2s - loss: 0.0459 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0.9891 - recall_m:  - ETA: 1s - loss: 0.0458 - accuracy: 0.9913 - f1_m: 0.9908 - precision_m: 0 - ETA: 1s - loss: 0.0456 - accuracy: 0.9913 - f1_m: 0.990 - ETA: 0s - loss: 0.0451 - accuracy: 0.9913 - f1_m: 0.9909 - precision_m: 0.9890 - reca\n",
      "Epoch 10/10\n",
      "21324/21324 [==============================] - 74s 3ms/step - loss: 0.0466 - accuracy: 0.9908 - f1_m: 0.9902 - precision_m: 0.9839 - recall_m: 0.9977 - val_loss: 0.0155 - val_accuracy: 0.9967 - val_f1_m: 0.9966 - val_precision_m: 0.9939 - val_recall_m: 0.9997ss: 0.0278 - accuracy: 0.9887 - f1_m: 0.9878 - precision_m: 0.9785 - recall - ETA: 1:01 - loss: 0.0280 - accuracy: 0.9892 - f1_m: 0.9884 - precision_m: - ETA: 1:00 - loss: 0.0291 - accuracy: 0.9897 - f1_m: 0.9888 - precision_m: 0 - ETA: 1:00 - loss: 0.0293 - accuracy: 0.9889 - f1_m: 0.9881 - precision_m: 0.9789 - recall - ETA: 1:00 - loss: 0.0287 - accuracy: 0.9891 - f1_m: 0.9884 - precision_m: 0.9794 - recall_m: 0. - ETA: 1:00 - loss: 0.0325 - accuracy: 0.9889 - f1_m: 0.9882 - precision_m: 0.9791 - recall - ETA: 1:00 - loss: 0.0329 - accuracy: 0.9888 - f1_m: 0.9880 - precision_m: 0.9788 - recall_m: 0.99 - ETA: 1:00 - loss: 0.0330 - accuracy: 0.9887 - f1_m: 0.9880 - precision_ - ETA: 59s - loss: 0.0336 - accuracy: 0.9885 - f1_m: 0.9877 - precision_m: 0.9782 - rec - ETA: 58s - loss: 0.0315 - accuracy: 0.9890 - f1_m: 0.9882 - precision_m: 0.9794 - recall_m:  - ETA: 58s - loss: 0.0309 - accuracy: 0.9891 - f1_m: 0.9883 - precision_m: 0.97 - ETA: 57s - loss: 0.0324 - accuracy: 0.9888 - f1_m: 0.9881 - precision_m: 0.9801 - recall - ETA: 56s - loss: 0.0324 - accuracy: 0.9889 - f1_m: 0.9882 - precision_m: 0.9813 - recall_m: 0. - ETA: 56s - loss: 0.0338 - accuracy: 0.9890 - f1_m: 0.9883 - precision_m: 0.9818 - reca - ETA: 55s - loss: 0.0337 - accuracy: 0.9890 - f1_m: 0.9883 - precision_m: 0.9815 - recall_ - ETA: 55s - loss: 0.0350 - accuracy: 0.9891 - f1_m: 0.9884 - precision_m: 0.9817 - recall_m: 0.996 - ETA: 55s - loss: 0.0351 - accuracy: 0.9891 - f1_m: 0.9884 - precision_m: 0.9817 - recall_ - ETA: 54s - loss: 0.0345 - accuracy: 0.9892 - f1_m: 0.9886 - precision_m: 0.9819 - recall_m - ETA: 54s - loss: 0.0346 - accuracy: 0.9893 - f1_m: 0.9887 - precision_m: 0.9820 - recall_m: 0.9 - ETA: 54s - loss: 0.0342 - accuracy: 0.9894 - f1_m: 0.9888 - precision_m: 0.9821 - recall_m - ETA: 53s - loss: 0.0352 - accuracy: 0.9893 - f1_m: 0.9887 - precision_m: 0.9818 - recall_m: 0.996 - ETA: 53s - loss: 0.0351 - accuracy: 0.9893 - f1_m: 0.9887 - precis - ETA: 52s - loss: 0.0718 - accuracy: 0.9891 - f1_m: 0.9881 - precision_m: 0.9812 - recall_m: 0.9 - ETA: 52s - loss: 0.0714 - accuracy: 0.9891 - f1_m: 0.9881 - precision - ETA: 50s - loss: 0.0665 - accuracy: 0.9892 - f1_m: 0.9883 - precision_m: 0.9812 - recall_ - ETA: 50s - loss: 0.0648 - accuracy: 0.9893 - f1_m: 0.9885 - precision_m: 0.98 - ETA: 49s - loss: 0.0613 - accuracy: 0.9897 - f1_m: 0.9889 - precision_m: 0.9820 - recall_m: 0. - ETA: 49s - loss: 0.0607 - accuracy: 0.9898 - f1_m: 0.9889 - precision_m: 0.9821 - recall_m: 0.99 - ETA: 48s - loss: 0.0605 - accuracy: 0.9898 - f1_m: 0.9889 - precision_m: 0.982 - ETA: 48s - loss: 0.0579 - accuracy: 0.9899 - f1_m: 0.9891 - precision_m: 0. - ETA: 47s - loss: 0.0555 - accuracy: 0.9901 - f1_m: 0.9893 - precision_m: 0.9826 - recall_m: 0 - ETA: 46s - loss: 0.0549 - accuracy: 0.9902 - f1_m: 0.9894 - precision_m: 0.9827 - r - ETA: 46s - loss: 0.0538 - accuracy: 0.9902 - f1_m: 0.9895 - precision_m: 0.9827 - recall_m: 0.997 - ETA: 46s - loss: 0.0537 - accuracy: 0.9902 - f1_m: 0.9895 - precision_m: 0.9828 - recall_m: 0.99 - ETA: 45s - loss: 0.0534 - accuracy: 0.9902 - f1_m: 0.9895 - precision_m: 0.9828 - recall_m - ETA: 45s - loss: 0.0525 - accuracy: 0.9904 - f1_m: 0.9896 - precision_m: 0.9829 - recall_m - ETA: 45s - loss: 0.0519 - accuracy: 0.9903 - f1_m: 0.9896 - precision_m: 0.9828 - recall_m: 0.997 - ETA: 45s - loss: 0.0518 - accuracy: 0.9903 - f1_m: 0.9896 - precision_m: 0.9828 - reca - ETA: 44s - loss: 0.0510 - accuracy: 0.9903 - f1_m: 0.9895 - precision_m: 0.9827 - recall_m: 0. - ETA: 44s - loss: 0.0508 - accuracy: 0.9903 - f1_m: 0.9895 - precision_m: 0.9827 - recall_m: 0. - ETA: 43s - loss: 0.0504 - accuracy: 0.9903 - f1_m: 0.9896 - precision_m: 0.9827 - recall_ - ETA: 43s - loss: 0.0497 - accuracy: 0.9904 - f1_m: 0.9896 - precision_m: 0.9828 - recall_m: 0.99 - ETA: 43s - loss: 0.0495 - accuracy: 0.9904 - f1_m: 0.9896 - precision_m: 0.9828 - reca - ETA: 42s - loss: 0.0499 - accuracy: 0.9904 - f1_m: 0.9896 - precision_m: 0.9828 - recall_m: 0.9 - ETA: 42s - loss: 0.0497 - accuracy: 0.9904 - f1_m: 0.9896 - precision_m: 0.9829 - recall_m: 0. - ETA: 42s - loss: 0.0494 - accuracy: 0.9904 - f1_m: 0.9896 - precision_m: 0.98 - ETA: 41s - loss: 0.0479 - accuracy: 0.9905 - f1_m: 0.9898 - precision_m: 0.9830 - recall_m: 0. - ETA: 40s - loss: 0.0478 - accuracy: 0.9905 - f1_m: 0.9898 - precision_m: 0.9831 - recall_m: 0 - ETA: 40s - loss: 0.0475 - accuracy: 0.9905 - f1_m: 0.9898 - precision_m: 0.9831 - r - ETA: 40s - loss: 0.0470 - accuracy: 0.9905 - f1_m: 0.9898 - precision_ - ETA: 38s - loss: 0.0467 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.9829 - recal - ETA: 37s - loss: 0.0461 - accuracy: 0.9904 - f1_m: 0.9897 - precision_m: 0.9828 - recall_m: 0. - ETA: 37s - loss: 0.0458 - accuracy: 0.9904 - f1_m: 0.9898 - precision_m: 0.9828 - rec - ETA: 36s - loss: 0.0450 - accuracy: 0.9905 - f1_m: 0.9898 - - ETA: 34s - loss: 0.0435 - accuracy: 0.9906 - f1_m - ETA: 32s - loss: 0.0422 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.9831 - recall_m: 0.998 - ETA: 32s - loss: 0.0422 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.9831 - recall_m: 0.9 - ETA: 32s - loss: 0.0422 - accuracy: 0.9906 - f1_m: 0.9900 - ETA: 30s - loss: 0.0412 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.9831 - recall_m:  - ETA: 30s - loss: 0.0415 - accuracy: 0.9907 - f1_m: 0.9900 - precision_m: 0.9832 - recall_m: - ETA: 29s - loss: 0.0415 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.9831 - - ETA: 28s - loss: 0.0409 - accuracy: 0.9907 - f1_m: 0.9901 - precision_m: 0.9831 - recall_m: 0 - ETA: 28s - loss: 0.0408 - accuracy: 0.9907 - f1_m: 0.9901 - - ETA: 26s - loss: 0.0414 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.9833 - recal - ETA: 25s - loss: 0.0414 - accuracy: 0.9906 - f1_m: 0.9900 - precision_m: 0.9834 - re - ETA: 25s - loss: 0.0410 - accuracy: 0.9907 - f1_m: 0.9901 - precision_m: 0.9 - ETA: 24s - loss: 0.0403 - accuracy: 0.9907 - f1_m: 0.9901 - precision_m: 0.9835 - recall_m: 0. - ETA: 23s - loss: 0.0402 - accuracy: 0.9907 - f1_m: 0.9901 - precision_m: 0.9835 - recall_m: 0. - ETA: 23s - loss: 0.0401 - accuracy: 0.9908 - f1_m: 0.9902 - precision_m: 0.9835 - recall_ - ETA: 23s - loss: 0.0398 - accuracy: 0.9908 - f1_m: 0.9902 - precision - ETA: 21s - loss: 0.0389 - accuracy: 0.9909 - f1_m: 0.9903 - precision_m: 0.9837  - ETA: 20s - loss: 0.0385 - accuracy: 0.9910 - f1_m: 0.9904 - precision_m: 0.9838 - recall_m: 0.9 - ETA: 20s - loss: 0.0384 - accuracy: 0.9910 - f1_m: 0.9904 - precision_m: 0.9838 - recall_m: - ETA: 20s - loss: 0.0382 - accuracy: 0.9910 - f1_m: 0.9905 - precision_m: 0.9839 - recall_m: - ETA: 19s - loss: 0.0380 - accuracy: 0.9911 - f1_m: 0.9905 - precision_m: 0.9840 - re - ETA: 19s - loss: 0.0389 - accuracy: 0.9910 - f1_m: 0.9904 - precision_m: 0.9839 - recall_m: - ETA: 18s - loss: 0.0388 - accuracy: 0.9910 - f1_m: 0.9904 - precision_m: 0.9840 - - ETA: 17s - loss: 0.0391 - accuracy: 0.9909 - f1_m: 0.9904 - precision_m: 0.9841 - recal - ETA: 17s - loss: 0.0397 - accuracy: 0.9909 - f1_m: 0.9903 - precision_m: 0.9841 - recall_m: 0. - ETA: 17s - loss: 0.0399 - accuracy: 0.9909 - f1_m: 0.9903 - precision_m: 0.9841 - recall_m - ETA: 16s - loss: 0.0397 - accuracy: 0.9909 - f1_m: 0.9903 - precision_m: 0.9841 -  - ETA: 15s - loss: 0.0395 - accuracy: 0.9910 - f1_m: 0.9904 - precision_m: 0.9842 -  - ETA: 15s - loss: 0.0393 - accuracy: 0.9910 - f1_m: 0.9904 - precision_m: 0.9843 - recall_m - ETA: 14s - loss: 0.0391 - accuracy: 0.9910 - f1_m: 0.9904 - precision_m: 0.9844  - ETA: 13s - loss: 0.0454 - accuracy: 0.9909 - f1_m: 0.9903 - precision_m: 0.9845 - recall_m: 0. - ETA: 13s - loss: 0.0454 - accuracy: 0.9909 - f1_m: 0.9903 - precision_m: 0.9845 - r - ETA: 12s - loss: 0.0454 - accuracy: 0.9909 - f1_m: 0.9903 - precision_m: 0.9844 - rec - ETA: 12s - loss: 0.0479 - accuracy: 0.9908 - f1_m: 0.9903 - precision_m: 0.9843 - recall_m: 0 - ETA: 12s - loss: 0.0478 - accuracy: 0.9908 - f1_m: 0.9903 - preci - ETA: 10s - loss: 0.0495 - accuracy: 0.9908 - f1_m: 0.9902 - precision_m: 0.9842 - recall_m: 0.9 - ETA: 10s - loss: 0.0496 - accuracy: 0.9908 - f1_m: 0.9902 - precision_m: 0.98 - ETA: 9s - loss: 0.0496 - accuracy: 0.9907 - f1_m: 0.9902 - prec - ETA: 8s - loss: 0.0494 - accuracy: 0.9907 - f - ETA: 4s - loss: 0.0483 - accuracy: 0.9908 - f1_m: 0.9902 - precision_m: 0.9840 - reca - E - ETA: 1s - loss: 0.0474 - accuracy: 0.9907 - f1_m: 0 - ETA: 0s - loss: 0.0469 - accuracy: 0.9907 - f1_m: 0.9902 - precision_\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ba51dee760>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compilamos el modelo\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-2), loss=\"binary_crossentropy\", metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    ") #Compilamos el modelo con su optimizador, la forma en la que actualizamos los pesos (minimización) y en base a qué métricas\n",
    "\n",
    "#Lo entrenamos con los datos de entrenamiento\n",
    "model.fit(\n",
    "    dataTrain,\n",
    "    targetTrain,\n",
    "    #batch_size=2048,\n",
    "    batch_size=20,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_data=(dataTest, targetTest),\n",
    ")#Conjuntos de entrenamientos y evaluación, numero de muestras en la propagación hacia atrás, \n",
    "#numero de iteraciones para mejorar el modelo, la verbosidad y los conjuntos de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos sobre el entrenamiento\n",
      "Exactitud de entrenamiento: 0.9968\n",
      "F1 de entrenamiento: 0.9967\n",
      "Precisión de entrenamiento: 0.9939\n",
      "Memoria de entrenamiento: 0.9997\n",
      "\n",
      "Datos sobre la evaluación\n",
      "Exactitud de evaluación: 0.9967\n",
      "F1 de evaluación: 0.9967\n",
      "Precisión de evaluación: 0.9939\n",
      "Memoria de evaluación: 0.9997\n"
     ]
    }
   ],
   "source": [
    "#Métricas\n",
    "#Conjunto de entrenamiento\n",
    "print(\"Datos sobre el entrenamiento\")\n",
    "loss, accuracy,f1_score, precision, recall = model.evaluate(dataTrain, targetTrain, verbose=False)\n",
    "print(\"Exactitud de entrenamiento: {:.4f}\".format(accuracy))\n",
    "print(\"F1 de entrenamiento: {:.4f}\".format(f1_score))\n",
    "print(\"Precisión de entrenamiento: {:.4f}\".format(precision))\n",
    "print(\"Memoria de entrenamiento: {:.4f}\".format(recall))\n",
    "\n",
    "#Conjunto de evaluación\n",
    "print()\n",
    "print(\"Datos sobre la evaluación\")\n",
    "loss, accuracy,f1_score, precision, recall = model.evaluate(dataTest, targetTest, verbose=False)\n",
    "print(\"Exactitud de evaluación: {:.4f}\".format(accuracy))\n",
    "print(\"F1 de evaluación: {:.4f}\".format(f1_score))\n",
    "print(\"Precisión de evaluación: {:.4f}\".format(precision))\n",
    "print(\"Memoria de evaluación: {:.4f}\".format(recall))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sobre las guías de usuario de estos 3 paquetes puede encontrar todas sus funcionalidades. En los siguientes código usaremos más funciones de ellos:\n",
    "- Scikit-learn: https://scikit-learn.org/stable/user_guide.html\n",
    "- Pandas: https://pandas.pydata.org/docs/user_guide/index.html\n",
    "- Numpy: https://numpy.org/doc/stable/\n",
    "- Imbalanced-learn: https://imbalanced-learn.readthedocs.io/en/stable/user_guide.html\n",
    "- Pickle: https://docs.python.org/3/library/pickle.html\n",
    "- Seaborn: https://seaborn.pydata.org/installing.html\n",
    "- Matplotlib: https://matplotlib.org/users/index.html\n",
    "- Keras: https://keras.io/guides/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e06ff7da33dc9620448857a90ad8b5f428f0d573d205a934d2841c8aee45ea32"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
