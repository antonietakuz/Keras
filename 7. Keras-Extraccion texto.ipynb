{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "*Antes de comenzar estas lineas de código debe instalar las ultimas versiones de los paquetes*\n",
    "- Scikit-learn: Buscando en anaconda scikit-learn o poniendo en el terminal conda install scikit-learn o pip install scikit-learn ---> Más info en https://scikit-learn.org/stable/install.html\n",
    "- Pandas: Buscando en anaconda pandas o poniendo en el terminal conda install pandas o pip install pandas ---> Más info en https://pandas.pydata.org/docs/getting_started/index.html\n",
    "- Numpy: Buscando en anaconda numpy o poniendo en el terminal conda install numpy o pip install numpy ---> Más info en https://numpy.org/install/\n",
    "- Imbalanced-learn: Poniendo en el terminal conda install -c conda-forge imbalanced-learn o pip install -U imbalanced-learn ---> Más info en https://imbalanced-learn.readthedocs.io/en/stable/install.html\n",
    "- Pickle: Poniendo en el terminal pip install pickle ---> Más info si no te funciona en https://stackoverflow.com/questions/48477949/not-able-to-pip-install-pickle-in-python-3-6/48477988\n",
    "- seaborn: Buscando en anaconda seaborn o poniendo en el terminal pip install seaborn o conda install seaborn ---> Más info en https://seaborn.pydata.org/installing.html\n",
    "- matplotlib: Buscando en anaconda matplotlib o poniendo en el terminal pip install -U matplotlib ---> Más info en https://matplotlib.org/users/installing.html\n",
    "- Keras: Buscando e instalando en anaconda los módulos de tensorflow y keras. También se pueden instalar usando los comandos pip install tensorflow y pip install keras ---> Más infor en https://keras.io/about/\n",
    "\n",
    "Base de datos obtenida de: https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura y balanceo de datos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lo primero de todo es descargarse la base de datos, guardamos la carpeta en el mismo directorio que nuestro código, y borramos todos los archivos menos las subcarpetas de pos y neg de las carpetas de train y test. Ahora leemos los datos.\n",
    "\n",
    "La base de datos es de análisis de sentimientos, siendo un 1 si este es positivo y un 0 si este es negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paquetes\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#Extracción de datos\n",
    "lote = 20000 #Numero de muestras con el que se trabaja hasta que se actualizan los parametros\n",
    "data_training = tf.keras.preprocessing.text_dataset_from_directory( #Se necesita TensorFlow version 2.3.0 o superior\n",
    "    \"aclImdb/train\", #Ruta de los datos\n",
    "    batch_size=lote, #Tamaño del lote\n",
    "    validation_split=0.2, #Proporcion de muestras de validacion\n",
    "    subset=\"training\", #Tipo de subset\n",
    "    seed=1337, #Semilla desde la que empiezo a coger muestras\n",
    ")\n",
    "data_validation = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=lote-15000,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "data_test = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=lote + 5000\n",
    ")\n",
    "\n",
    "#Número de lotes\n",
    "print(\n",
    "    \"Número de lotes de entrenamiento: %d\"\n",
    "    % tf.data.experimental.cardinality(data_training)\n",
    ")\n",
    "print(\n",
    "    \"Número de lotes de validación: %d\" % tf.data.experimental.cardinality(data_validation)\n",
    ")\n",
    "print(\n",
    "    \"Número de lotes de prueba: %d\"\n",
    "    % tf.data.experimental.cardinality(data_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Visualizamos los datos contenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto:  b\"I was really looking forward to this movie based on the previews and press it received, but after viewing it I was terribly disappointed. Slums is a totally unfunny film mixed with a Todd Solondz type of disturbing family reality sans Todd's brand of humor. The story drags along and each scene is worse than the last. Maybe I missed the point, but if this film is an example of every girl's growing up experience I am glad to be a man.\"\n",
      "Etiqueta:  0\n",
      "Texto:  b'This movie is likely the worst movie I\\'ve ever seen in my life -- surpassing the previous most god-awful movie, \"Spawn of Slithis,\" which I saw when I was about 10.<br /><br />Bad acting, stilted and ridiculous dialog, incomprehensible plot, mishmashed cut scenes, even the music was annoying. Did I leave anything out? Well, the special effects weren\\'t bad -- but CGI does not a decent movie make.<br /><br />I can\\'t believe I actually spent money to see this movie. If anyone has the contact info for Hyung-rae Shim (the director), please forward it to my user name \"at gmail,\" and I\\'ll contact him to personally demand a refund.'\n",
      "Etiqueta:  0\n",
      "Texto:  b\"I found this episode to be one of funniest I've seen in a long time. The south park creators have done the best spoof of a Romero film I have ever seen.They have truly touched on Romero's underlying social commentary that he has made with each one of his films. I would love to know what George Romero's opinion was on this episode I'm sure it was purely positive! Keeping his true vision for his zombie epics fully intact! Most spoofs deal with the pure gore without making the viewer think as Romero tries to do with his films. I think that if a zombie outbreak did happen we may actually worry about our property values before our lives as shown in this episode!\"\n",
      "Etiqueta:  1\n",
      "Texto:  b\"THE KING MAKER will doubtless be a success in Thailand where the similar (but superior) 'The Legend of Suriyothai' set box office records. The film directed by Lek Kitaparaporn after a screenplay by Sean Casey based on historical fact in 1547 Siam has some amazingly beautiful visual elements but is disarmed by one of the corniest, pedestrian scripts and story development on film.<br /><br />The event the picture relates is the arrival of the Portuguese soldier of fortune Fernando de Gamma (Gary Stretch) whose vengeance for this father's murderer drives him to shipwrecked, captured and thrown into slavery and put on the bloc in Ayutthaya in the kingdom of Siam where he is purchased by the beautiful Maria (Cindy Burbridge) with the consent of her father Phillipe (John Rhys-Davies), a man with a name and a past that are revealed as the story progresses. There is a plot to overthrown the King and Fernando and his new Siamese sidekick Tong (Dom Hetrakul), after some gratuitous CGI enhanced choreographed martial arts silliness, are first rewarded by the King to become his bodyguards, only to be imprisoned together once Queen Sudachan (Yoe Hassadeevichit) reveals her plot to kill the king and son to allow her lover Lord Chakkraphat (Oliver Pupart) to take over the rule of Siam. Yet of course Fernando and Tong escape and are condemned to fight each other to save the lives of their families (Tong's wife and children and Fernando's now firm love affair with Maria) with the expected consequences.<br /><br />The acting (with the exception of John Rhys-Davies) is so weak that the film occasionally seems as though it were meant to be camp. The predominantly Thai cast struggle with the poorly written dialog, making us wish they had used their native Thai with subtitles. The musical score by Ian Livingstone sounds as though exhumed form old TV soap operas. But if it is visual splendor you're after there is plenty of that and that alone makes the movie worth watching. It is a film that has obvious high financial backing for all the special effects and masses of cast and sets and shows its good intentions. It is just the basics that are missing. Grady Harp\"\n",
      "Etiqueta:  0\n",
      "Texto:  b'Joan Fontaine is \"A Damsel in Distress\" in this 1937 musical starring Fred Astaire, George Burns, and Gracie Allen. The plot, what there is of it, is about a British woman (Fontaine) in love with an American, who is mistaken for Astaire, a musical comedy star.<br /><br />The film, directed by George Stevens, contains some wonderful Gershwin music, including \"Nice Work if You Can Get It\" and \"A Foggy Day.\" The best scene is the \"Stiff Upper Lip\" number, which takes place in a fun house.<br /><br />Astaire\\'s singing voice sounds more robust in this film than it does in others, and he has a couple of excellent dance numbers. Burns plays his over the top publicist and Allen is Burns\\' secretary. She\\'s hilarious. The problem, as others have pointed out, is Fontaine, who has to dance with Astaire at the end of the film. Stevens could easily have used a double because he shows the dance in a long shot, and it takes place among the trees. I would have thought it was a double except the dancing was so lousy.<br /><br />Definitely worth seeing despite its flaws.'\n",
      "Etiqueta:  1\n"
     ]
    }
   ],
   "source": [
    "#Bucle para extraer los valores\n",
    "for texto, etiqueta in data_training.take(1):\n",
    "    for i in range(5):\n",
    "        print('Texto: ',texto.numpy()[i])\n",
    "        print('Etiqueta: ',etiqueta.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paquetes\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import \\\n",
    "    TextVectorization\n",
    "\n",
    "\n",
    "# Creamos una función para eliminar las / de nuestro texto, y poner todas las letras en minúsculas\n",
    "def preparacion(texto):\n",
    "    texto_minusculas = tf.strings.lower(texto)\n",
    "    sin_barras = tf.strings.regex_replace(texto_minusculas, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        sin_barras, \"[%s]\" % re.escape(string.punctuation), \"\")\n",
    "\n",
    "\n",
    "# Definimos la vectorización.\n",
    "caracteristicas = 20000 #Número máximo de tokens de la capa (con cuantas palabras trabaja)\n",
    "long_seq = 500 #Longitud máxima de cada secuencia\n",
    "\n",
    "# Definimos la capa de vectorizacion\n",
    "capa_vectorizacion = TextVectorization(\n",
    "    #Definimos los valores\n",
    "    standardize=preparacion,\n",
    "    max_tokens=caracteristicas,\n",
    "    output_mode=\"int\", #El vector que sale es de enteros\n",
    "    output_sequence_length=long_seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Elaboramos los vectores de datos que se introducirán en la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizacion(texto, etiqueta):\n",
    "    texto = tf.expand_dims(texto, -1)\n",
    "    return capa_vectorizacion(texto), etiqueta\n",
    "\n",
    "\n",
    "# Vectorize the data.\n",
    "training = data_training.map(vectorizacion)\n",
    "validation = data_validation.map(vectorizacion)\n",
    "test = data_test.map(vectorizacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Etiqueta:  0\n",
      "Texto:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Etiqueta:  1\n",
      "Texto:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Etiqueta:  1\n",
      "Texto:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Etiqueta:  1\n",
      "Texto:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Etiqueta:  1\n"
     ]
    }
   ],
   "source": [
    "#Volvemos a extraer los valores con el mismo bucle\n",
    "for texto, etiqueta in training.take(1):\n",
    "    for i in range(5):\n",
    "        print('Texto: ',texto.numpy()[i])\n",
    "        print('Etiqueta: ',etiqueta.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reseñas positivas: 9976\n",
      "Reseñas negativas: 10024\n"
     ]
    }
   ],
   "source": [
    "#Primero pasamos los arrays a numpy como estamos acostumbrados\n",
    "for texto, etiqueta in training.take(1):\n",
    "    dataTraining = texto.numpy()\n",
    "    targetTraining = etiqueta.numpy()\n",
    "\n",
    "for texto, etiqueta in validation.take(1):\n",
    "    dataValidation = texto.numpy()\n",
    "    targetValidation = etiqueta.numpy()\n",
    "    \n",
    "for texto, etiqueta in test.take(1):\n",
    "    dataTest = texto.numpy()\n",
    "    targetTest = etiqueta.numpy()\n",
    "    \n",
    "#Después contamos los valores positivos y negativos que tengo en el entrenamiento\n",
    "\n",
    "#Inicializacion de la cuenta\n",
    "Positivo = 0\n",
    "Negativo = 0\n",
    "\n",
    "#Cuenta de datos\n",
    "for x in range(targetTraining.shape[0]):\n",
    "    if targetTraining[x] == 0:\n",
    "        Negativo = Negativo + 1\n",
    "    else:\n",
    "        Positivo = Positivo + 1\n",
    "\n",
    "#Representación\n",
    "print(\"Reseñas positivas: \" + str(Positivo))\n",
    "print(\"Reseñas negativas: \" + str(Negativo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTraining.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reseñas positivas: 9976\n",
      "Reseñas negativas: 9976\n"
     ]
    }
   ],
   "source": [
    "#Importamos los paquetes de subremuestreo\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "#SMOTE\n",
    "NM = NearMiss()\n",
    "\n",
    "#Generación de nuevas muestras sintéticas\n",
    "dataTrainingNM, tragetTrainingNM = NM.fit_resample(dataTraining, targetTraining)\n",
    "\n",
    "#Inicializacion de la cuenta\n",
    "Positivo = 0\n",
    "Negativo = 0\n",
    "\n",
    "#Cuenta de datos\n",
    "for x in range(tragetTrainingNM.shape[0]):\n",
    "    if tragetTrainingNM[x] == 0:\n",
    "        Negativo = Negativo + 1\n",
    "    else:\n",
    "        Positivo = Positivo + 1\n",
    "\n",
    "#Representación\n",
    "print(\"Reseñas positivas: \" + str(Positivo))\n",
    "print(\"Reseñas negativas: \" + str(Negativo))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Aunque existen otras formas de solucionar problemas de bases de datos no balanceadas como normalizar los datos, filtrar algunos de ellos..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sobre las guías de usuario de estos 3 paquetes puede encontrar todas sus funcionalidades. En los siguientes código usaremos más funciones de ellos:\n",
    "- Scikit-learn: https://scikit-learn.org/stable/user_guide.html\n",
    "- Pandas: https://pandas.pydata.org/docs/user_guide/index.html\n",
    "- Numpy: https://numpy.org/doc/stable/\n",
    "- Imbalanced-learn: https://imbalanced-learn.readthedocs.io/en/stable/user_guide.html\n",
    "- Pickle: https://docs.python.org/3/library/pickle.html\n",
    "- Seaborn: https://seaborn.pydata.org/installing.html\n",
    "- Matplotlib: https://matplotlib.org/users/index.html\n",
    "- Keras: https://keras.io/guides/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('RetinanetPrueba1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "38299c45378107576d757ae9c390f9c4c1a9d1a3694580b4313f8e46a2346776"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
